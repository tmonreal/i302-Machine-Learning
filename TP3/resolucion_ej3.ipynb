{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car.csv')\n",
    "\n",
    "# Dividir en conjunto de entrenamiento (70%), validación (15%) y prueba (15%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['acceptability'], random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['acceptability'], random_state=42)\n",
    "\n",
    "# Guardar los conjuntos en archivos CSV\n",
    "train_df.to_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_train.csv', index=False)\n",
    "valid_df.to_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_valid.csv', index=False)\n",
    "test_df.to_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor valor de C encontrado: 100\n",
      "F1-score macro en el conjunto de validación: 0.8961993922468231\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar los datasets\n",
    "train_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_train.csv')\n",
    "valid_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_valid.csv')\n",
    "test_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_test.csv')\n",
    "\n",
    "# Obtener características y etiquetas\n",
    "X_train = train_df.drop('acceptability', axis=1)\n",
    "y_train = train_df['acceptability']\n",
    "X_valid = valid_df.drop('acceptability', axis=1)\n",
    "y_valid = valid_df['acceptability']\n",
    "\n",
    "# Preprocesamiento de las etiquetas utilizando LabelEncoder\n",
    "lab_enc = LabelEncoder()\n",
    "lab_enc.fit(y_train)\n",
    "y_train_encoded = lab_enc.transform(y_train)\n",
    "y_valid_encoded = lab_enc.transform(y_valid)\n",
    "\n",
    "# Preprocesamiento de características utilizando pd.get_dummies()\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_valid_encoded = pd.get_dummies(X_valid, drop_first=True)\n",
    "\n",
    "# Definir los valores de lambda a probar\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "lambda_values = [1 / C for C in param_grid['C']]\n",
    "\n",
    "# Inicializar el modelo de regresión logística\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Realizar búsqueda en la cuadrícula para encontrar el mejor valor de C\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Obtener el mejor valor de C\n",
    "best_C = grid_search.best_params_['C']\n",
    "\n",
    "# Entrenar el modelo de regresión logística con el mejor valor de C\n",
    "best_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, C=best_C)\n",
    "best_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Predecir las etiquetas en el conjunto de validación\n",
    "y_pred = best_model.predict(X_valid_encoded)\n",
    "\n",
    "# Calcular el f1-score en el conjunto de validación\n",
    "f1_macro = f1_score(y_valid_encoded, y_pred, average='macro')\n",
    "\n",
    "print(\"El mejor valor de C encontrado:\", best_C)\n",
    "print(\"F1-score macro en el conjunto de validación:\", f1_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 49   1   5   2]\n",
      " [  1  10   0   0]\n",
      " [  4   0 177   0]\n",
      " [  0   1   0   9]]\n",
      "\n",
      "Accuracy: 0.9459459459459459\n",
      "\n",
      "Precision por clase: [0.90740741 0.83333333 0.97252747 0.81818182]\n",
      "\n",
      "Recall por clase: [0.85964912 0.90909091 0.97790055 0.9       ]\n",
      "\n",
      "F1-score por clase: [0.88288288 0.86956522 0.97520661 0.85714286]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar los datasets\n",
    "train_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_train.csv')\n",
    "valid_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_valid.csv')\n",
    "test_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_test.csv')\n",
    "\n",
    "# Obtener características y etiquetas\n",
    "X_train = train_df.drop('acceptability', axis=1)\n",
    "y_train = train_df['acceptability']\n",
    "X_valid = valid_df.drop('acceptability', axis=1)\n",
    "y_valid = valid_df['acceptability']\n",
    "\n",
    "# Preprocesamiento de las etiquetas utilizando LabelEncoder\n",
    "lab_enc = LabelEncoder()\n",
    "lab_enc.fit(y_train)\n",
    "y_train_encoded = lab_enc.transform(y_train)\n",
    "y_valid_encoded = lab_enc.transform(y_valid)\n",
    "\n",
    "# Preprocesamiento de características utilizando pd.get_dummies()\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_valid_encoded = pd.get_dummies(X_valid, drop_first=True)\n",
    "\n",
    "# Definir los valores de lambda a probar\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "lambda_values = [1 / C for C in param_grid['C']]\n",
    "\n",
    "# Inicializar el modelo de regresión logística\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Realizar búsqueda en la cuadrícula para encontrar el mejor valor de C\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Obtener el mejor valor de C\n",
    "best_C = grid_search.best_params_['C']\n",
    "\n",
    "# Entrenar el modelo de regresión logística con el mejor valor de C\n",
    "best_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, C=best_C)\n",
    "best_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Predecir las etiquetas en el conjunto de validación\n",
    "y_pred = best_model.predict(X_valid_encoded)\n",
    "\n",
    "# Calcular las métricas de evaluación\n",
    "conf_matrix = confusion_matrix(y_valid_encoded, y_pred)\n",
    "accuracy = accuracy_score(y_valid_encoded, y_pred)\n",
    "precision = precision_score(y_valid_encoded, y_pred, average=None)\n",
    "recall = recall_score(y_valid_encoded, y_pred, average=None)\n",
    "f1 = f1_score(y_valid_encoded, y_pred, average=None)\n",
    "\n",
    "# Imprimir la matriz de confusión y las métricas de evaluación por clase\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nPrecision por clase:\", precision)\n",
    "print(\"\\nRecall por clase:\", recall)\n",
    "print(\"\\nF1-score por clase:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR) per class on the validation set:\n",
      "Class 0: 85.96%\n",
      "Class 1: 90.91%\n",
      "Class 2: 97.79%\n",
      "Class 3: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positive Rate (TPR) per class\n",
    "def true_positive_rate_per_class(conf_matrix):\n",
    "    tpr_per_class = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        true_positive = conf_matrix[i, i]\n",
    "        false_negative = np.sum(conf_matrix[i, :]) - true_positive\n",
    "        tpr = true_positive / (true_positive + false_negative)\n",
    "        tpr_per_class.append(tpr)\n",
    "    return tpr_per_class\n",
    "\n",
    "# Calculate True Positive Rate (TPR) per class on the validation set\n",
    "tpr_valid = true_positive_rate_per_class(conf_matrix)\n",
    "print(\"True Positive Rate (TPR) per class on the validation set:\")\n",
    "for i, tpr in enumerate(tpr_valid):\n",
    "    print(f\"Class {i}: {tpr * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión en el conjunto de prueba:\n",
      "[[ 53   0   4   1]\n",
      " [  2   8   0   0]\n",
      " [  7   0 175   0]\n",
      " [  0   0   0  10]]\n",
      "\n",
      "Accuracy en el conjunto de prueba: 0.9461538461538461\n",
      "\n",
      "Precision por clase en el conjunto de prueba: [0.85483871 1.         0.97765363 0.90909091]\n",
      "\n",
      "Recall por clase en el conjunto de prueba: [0.9137931  0.8        0.96153846 1.        ]\n",
      "\n",
      "F1-score por clase en el conjunto de prueba: [0.88333333 0.88888889 0.96952909 0.95238095]\n",
      "\n",
      "Comparación con las métricas reportadas en el conjunto de validación:\n",
      "Diferencia en Accuracy: 0.00020790020790018016\n",
      "Diferencia en Precision por clase: [-0.0525687   0.16666667  0.00512616  0.09090909]\n",
      "Diferencia en Recall por clase: [ 0.05414398 -0.10909091 -0.01636209  0.1       ]\n",
      "Diferencia en F1-score por clase: [ 0.00045045  0.01932367 -0.00567753  0.0952381 ]\n"
     ]
    }
   ],
   "source": [
    "# Obtener características y etiquetas del conjunto de prueba\n",
    "X_test = test_df.drop('acceptability', axis=1)\n",
    "y_test = test_df['acceptability']\n",
    "\n",
    "# Preprocesamiento de las etiquetas utilizando LabelEncoder\n",
    "y_test_encoded = lab_enc.transform(y_test)\n",
    "\n",
    "# Preprocesamiento de características utilizando pd.get_dummies()\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Predecir las etiquetas en el conjunto de prueba\n",
    "y_pred_test = best_model.predict(X_test_encoded)\n",
    "\n",
    "# Calcular las métricas de evaluación en el conjunto de prueba\n",
    "conf_matrix_test = confusion_matrix(y_test_encoded, y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test_encoded, y_pred_test)\n",
    "precision_test = precision_score(y_test_encoded, y_pred_test, average=None)\n",
    "recall_test = recall_score(y_test_encoded, y_pred_test, average=None)\n",
    "f1_test = f1_score(y_test_encoded, y_pred_test, average=None)\n",
    "\n",
    "# Imprimir la matriz de confusión y las métricas de evaluación por clase en el conjunto de prueba\n",
    "print(\"Matriz de Confusión en el conjunto de prueba:\")\n",
    "print(conf_matrix_test)\n",
    "print(\"\\nAccuracy en el conjunto de prueba:\", accuracy_test)\n",
    "print(\"\\nPrecision por clase en el conjunto de prueba:\", precision_test)\n",
    "print(\"\\nRecall por clase en el conjunto de prueba:\", recall_test)\n",
    "print(\"\\nF1-score por clase en el conjunto de prueba:\", f1_test)\n",
    "\n",
    "# Comparación con las métricas reportadas en el conjunto de validación\n",
    "print(\"\\nComparación con las métricas reportadas en el conjunto de validación:\")\n",
    "print(\"Diferencia en Accuracy:\", accuracy_test - accuracy)\n",
    "print(\"Diferencia en Precision por clase:\", precision_test - precision)\n",
    "print(\"Diferencia en Recall por clase:\", recall_test - recall)\n",
    "print(\"Diferencia en F1-score por clase:\", f1_test - f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Positive Rate (TPR) per class on the test set:\n",
      "Class 0: 91.38%\n",
      "Class 1: 80.00%\n",
      "Class 2: 96.15%\n",
      "Class 3: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positive Rate (TPR) per class on the test set\n",
    "tpr_test = true_positive_rate_per_class(conf_matrix_test)\n",
    "print(\"\\nTrue Positive Rate (TPR) per class on the test set:\")\n",
    "for i, tpr in enumerate(tpr_test):\n",
    "    print(f\"Class {i}: {tpr * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 48   2   7   0]\n",
      " [  5   5   0   1]\n",
      " [  9   0 172   0]\n",
      " [  1   1   0   8]]\n",
      "\n",
      "Accuracy: 0.8996138996138996\n",
      "\n",
      "Precision por clase: [0.76190476 0.625      0.96089385 0.88888889]\n",
      "\n",
      "Recall por clase: [0.84210526 0.45454545 0.95027624 0.8       ]\n",
      "\n",
      "F1-score por clase: [0.8        0.52631579 0.95555556 0.84210526]\n",
      "\n",
      "Matriz de Confusión en el conjunto de prueba:\n",
      "[[ 51   1   6   0]\n",
      " [  6   4   0   0]\n",
      " [  2   0 180   0]\n",
      " [  0   0   0  10]]\n",
      "\n",
      "Accuracy en el conjunto de prueba: 0.9423076923076923\n",
      "\n",
      "Precision por clase en el conjunto de prueba: [0.86440678 0.8        0.96774194 1.        ]\n",
      "\n",
      "Recall por clase en el conjunto de prueba: [0.87931034 0.4        0.98901099 1.        ]\n",
      "\n",
      "F1-score por clase en el conjunto de prueba: [0.87179487 0.53333333 0.97826087 1.        ]\n",
      "\n",
      "Comparación con las métricas reportadas en el conjunto de validación:\n",
      "Diferencia en Accuracy: 0.0426937926937927\n",
      "Diferencia en Precision por clase: [0.10250202 0.175      0.00684808 0.11111111]\n",
      "Diferencia en Recall por clase: [ 0.03720508 -0.05454545  0.03873475  0.2       ]\n",
      "Diferencia en F1-score por clase: [0.07179487 0.00701754 0.02270531 0.15789474]\n",
      "True Positive Rate (TPR) per class on the validation set:\n",
      "Class 0: 84.21%\n",
      "Class 1: 45.45%\n",
      "Class 2: 95.03%\n",
      "Class 3: 80.00%\n",
      "\n",
      "True Positive Rate (TPR) per class on the test set:\n",
      "Class 0: 87.93%\n",
      "Class 1: 40.00%\n",
      "Class 2: 98.90%\n",
      "Class 3: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar los datasets\n",
    "train_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_train.csv')\n",
    "valid_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_valid.csv')\n",
    "test_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_test.csv')\n",
    "\n",
    "# Obtener características y etiquetas\n",
    "X_train = train_df.drop('acceptability', axis=1)\n",
    "y_train = train_df['acceptability']\n",
    "X_valid = valid_df.drop('acceptability', axis=1)\n",
    "y_valid = valid_df['acceptability']\n",
    "\n",
    "# Preprocesamiento de las etiquetas utilizando LabelEncoder\n",
    "lab_enc = LabelEncoder()\n",
    "lab_enc.fit(y_train)\n",
    "y_train_encoded = lab_enc.transform(y_train)\n",
    "y_valid_encoded = lab_enc.transform(y_valid)\n",
    "\n",
    "# Preprocesamiento de características utilizando pd.get_dummies()\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_valid_encoded = pd.get_dummies(X_valid, drop_first=True)\n",
    "\n",
    "# Definir los hiperparámetros a probar\n",
    "param_grid = {'max_depth': [None, 5, 10, 15, 20], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Inicializar el modelo de árbol de decisión\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Realizar búsqueda en la cuadrícula para encontrar los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Entrenar el modelo de árbol de decisión con los mejores hiperparámetros\n",
    "best_model = DecisionTreeClassifier(**best_params)\n",
    "best_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Predecir las etiquetas en el conjunto de validación\n",
    "y_pred = best_model.predict(X_valid_encoded)\n",
    "\n",
    "# Calcular las métricas de evaluación\n",
    "conf_matrix = confusion_matrix(y_valid_encoded, y_pred)\n",
    "accuracy = accuracy_score(y_valid_encoded, y_pred)\n",
    "precision = precision_score(y_valid_encoded, y_pred, average=None)\n",
    "recall = recall_score(y_valid_encoded, y_pred, average=None)\n",
    "f1 = f1_score(y_valid_encoded, y_pred, average=None)\n",
    "\n",
    "# Imprimir la matriz de confusión y las métricas de evaluación por clase\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nPrecision por clase:\", precision)\n",
    "print(\"\\nRecall por clase:\", recall)\n",
    "print(\"\\nF1-score por clase:\", f1)\n",
    "\n",
    "# Obtener características y etiquetas del conjunto de prueba\n",
    "X_test = test_df.drop('acceptability', axis=1)\n",
    "y_test = test_df['acceptability']\n",
    "\n",
    "# Preprocesamiento de características utilizando pd.get_dummies()\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Predecir las etiquetas en el conjunto de prueba\n",
    "y_pred_test = best_model.predict(X_test_encoded)\n",
    "\n",
    "# Preprocesamiento de las etiquetas utilizando LabelEncoder\n",
    "y_test_encoded = lab_enc.transform(y_test)\n",
    "\n",
    "# Calcular las métricas de evaluación en el conjunto de prueba\n",
    "conf_matrix_test = confusion_matrix(y_test_encoded, y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test_encoded, y_pred_test)\n",
    "precision_test = precision_score(y_test_encoded, y_pred_test, average=None)\n",
    "recall_test = recall_score(y_test_encoded, y_pred_test, average=None)\n",
    "f1_test = f1_score(y_test_encoded, y_pred_test, average=None)\n",
    "\n",
    "# Imprimir la matriz de confusión y las métricas de evaluación por clase en el conjunto de prueba\n",
    "print(\"\\nMatriz de Confusión en el conjunto de prueba:\")\n",
    "print(conf_matrix_test)\n",
    "print(\"\\nAccuracy en el conjunto de prueba:\", accuracy_test)\n",
    "print(\"\\nPrecision por clase en el conjunto de prueba:\", precision_test)\n",
    "print(\"\\nRecall por clase en el conjunto de prueba:\", recall_test)\n",
    "print(\"\\nF1-score por clase en el conjunto de prueba:\", f1_test)\n",
    "\n",
    "# Comparación con las métricas reportadas en el conjunto de validación\n",
    "print(\"\\nComparación con las métricas reportadas en el conjunto de validación:\")\n",
    "print(\"Diferencia en Accuracy:\", accuracy_test - accuracy)\n",
    "print(\"Diferencia en Precision por clase:\", precision_test - precision)\n",
    "print(\"Diferencia en Recall por clase:\", recall_test - recall)\n",
    "print(\"Diferencia en F1-score por clase:\", f1_test - f1)\n",
    "\n",
    "# Calculate True Positive Rate (TPR) per class on the validation set\n",
    "tpr_valid = true_positive_rate_per_class(conf_matrix)\n",
    "print(\"True Positive Rate (TPR) per class on the validation set:\")\n",
    "for i, tpr in enumerate(tpr_valid):\n",
    "    print(f\"Class {i}: {tpr * 100:.2f}%\")\n",
    "\n",
    "# Calculate True Positive Rate (TPR) per class on the test set\n",
    "tpr_test = true_positive_rate_per_class(conf_matrix_test)\n",
    "print(\"\\nTrue Positive Rate (TPR) per class on the test set:\")\n",
    "for i, tpr in enumerate(tpr_test):\n",
    "    print(f\"Class {i}: {tpr * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics on the validation set:\n",
      "Confusion Matrix:\n",
      "[[ 56   0   1   0]\n",
      " [  0  11   0   0]\n",
      " [  0   0 181   0]\n",
      " [  0   0   0  10]]\n",
      "Accuracy: 0.9961389961389961\n",
      "Precision per class: [1.         1.         0.99450549 1.        ]\n",
      "Recall per class: [0.98245614 1.         1.         1.        ]\n",
      "F1-score per class: [0.99115044 1.         0.99724518 1.        ]\n",
      "\n",
      "Evaluation metrics on the test set:\n",
      "Confusion Matrix:\n",
      "[[ 56   0   2   0]\n",
      " [  0  10   0   0]\n",
      " [  1   0 181   0]\n",
      " [  0   0   0  10]]\n",
      "Accuracy: 0.9884615384615385\n",
      "Precision per class: [0.98245614 1.         0.98907104 1.        ]\n",
      "Recall per class: [0.96551724 1.         0.99450549 1.        ]\n",
      "F1-score per class: [0.97391304 1.         0.99178082 1.        ]\n",
      "\n",
      "True Positive Rate (TPR) per class on the validation set:\n",
      "Class 0: 98.25%\n",
      "Class 1: 100.00%\n",
      "Class 2: 100.00%\n",
      "Class 3: 100.00%\n",
      "\n",
      "True Positive Rate (TPR) per class on the test set:\n",
      "Class 0: 96.55%\n",
      "Class 1: 100.00%\n",
      "Class 2: 99.45%\n",
      "Class 3: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to calculate True Positive Rate (TPR) per class\n",
    "def true_positive_rate_per_class(conf_matrix):\n",
    "    tpr_per_class = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        true_positive = conf_matrix[i, i]\n",
    "        false_negative = np.sum(conf_matrix[i, :]) - true_positive\n",
    "        tpr = true_positive / (true_positive + false_negative)\n",
    "        tpr_per_class.append(tpr)\n",
    "    return tpr_per_class\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_train.csv')\n",
    "valid_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_valid.csv')\n",
    "test_df = pd.read_csv('/home/linar/Desktop/ML/Clases/Clase 3/i302/TP3/Data/3 - Vehicle Acceptability Evaluation/car_test.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df.drop('acceptability', axis=1)\n",
    "y_train = train_df['acceptability']\n",
    "X_valid = valid_df.drop('acceptability', axis=1)\n",
    "y_valid = valid_df['acceptability']\n",
    "\n",
    "# Preprocess labels using LabelEncoder\n",
    "lab_enc = LabelEncoder()\n",
    "lab_enc.fit(y_train)\n",
    "y_train_encoded = lab_enc.transform(y_train)\n",
    "y_valid_encoded = lab_enc.transform(y_valid)\n",
    "\n",
    "# Preprocess features using pd.get_dummies()\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_valid_encoded = pd.get_dummies(X_valid, drop_first=True)\n",
    "\n",
    "# Define MLPClassifier parameters\n",
    "mlp_params = {'hidden_layer_sizes': (100, 50), 'max_iter': 1000, 'random_state': 1}\n",
    "\n",
    "# Initialize MLPClassifier\n",
    "model = MLPClassifier(**mlp_params)\n",
    "\n",
    "# Train MLPClassifier\n",
    "model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Predict labels on the validation set\n",
    "y_pred_valid = model.predict(X_valid_encoded)\n",
    "\n",
    "# Calculate evaluation metrics on the validation set\n",
    "conf_matrix_valid = confusion_matrix(y_valid_encoded, y_pred_valid)\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_pred_valid)\n",
    "precision_valid = precision_score(y_valid_encoded, y_pred_valid, average=None)\n",
    "recall_valid = recall_score(y_valid_encoded, y_pred_valid, average=None)\n",
    "f1_valid = f1_score(y_valid_encoded, y_pred_valid, average=None)\n",
    "\n",
    "# Print evaluation metrics on the validation set\n",
    "print(\"Evaluation metrics on the validation set:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_valid)\n",
    "print(\"Accuracy:\", accuracy_valid)\n",
    "print(\"Precision per class:\", precision_valid)\n",
    "print(\"Recall per class:\", recall_valid)\n",
    "print(\"F1-score per class:\", f1_valid)\n",
    "\n",
    "# Extract features and labels from the test set\n",
    "X_test = test_df.drop('acceptability', axis=1)\n",
    "y_test = test_df['acceptability']\n",
    "\n",
    "# Preprocess features using pd.get_dummies()\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Predict labels on the test set\n",
    "y_pred_test = model.predict(X_test_encoded)\n",
    "\n",
    "# Preprocess labels using LabelEncoder\n",
    "y_test_encoded = lab_enc.transform(y_test)\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "conf_matrix_test = confusion_matrix(y_test_encoded, y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test_encoded, y_pred_test)\n",
    "precision_test = precision_score(y_test_encoded, y_pred_test, average=None)\n",
    "recall_test = recall_score(y_test_encoded, y_pred_test, average=None)\n",
    "f1_test = f1_score(y_test_encoded, y_pred_test, average=None)\n",
    "\n",
    "# Print evaluation metrics on the test set\n",
    "print(\"\\nEvaluation metrics on the test set:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_test)\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision per class:\", precision_test)\n",
    "print(\"Recall per class:\", recall_test)\n",
    "print(\"F1-score per class:\", f1_test)\n",
    "\n",
    "# Calculate True Positive Rate (TPR) per class on the validation set\n",
    "tpr_valid = true_positive_rate_per_class(conf_matrix_valid)\n",
    "print(\"\\nTrue Positive Rate (TPR) per class on the validation set:\")\n",
    "for i, tpr in enumerate(tpr_valid):\n",
    "    print(f\"Class {i}: {tpr * 100:.2f}%\")\n",
    "\n",
    "# Calculate True Positive Rate (TPR) per class on the test set\n",
    "tpr_test = true_positive_rate_per_class(conf_matrix_test)\n",
    "print(\"\\nTrue Positive Rate (TPR) per class on the test set:\")\n",
    "for i, tpr in enumerate(tpr_test):\n",
    "    print(f\"Class {i}: {tpr * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i302",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
