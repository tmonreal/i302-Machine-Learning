{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos - Splits \n",
    "Imagenes extraidas de [Rashcka, S., \"Model Evaluation, Model Selection, and Algorithm\n",
    "Selection in Machine Learning\", (2018)](https://arxiv.org/pdf/1811.12808.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo común es que trabajemos los datasets en formato de **Pandas DataFrame**. Este objeto podemos pensarlo como un conjunto de sub-objetos llamados **Series** que combinan la versatilidad de un array de numpy con otras propiedades de tablas propias del dataframe. De esta manera, es sencillo pensar a nuestro dataset de forma tabular, y explotar esta representación para definir splits, analizar propiedades presenta en la metadata de los datos, y sobre todo para escribir/leer estas listas de manera eficiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width species\n",
       "0           1.4          0.2  setosa\n",
       "1           1.4          0.2  setosa\n",
       "2           1.3          0.2  setosa\n",
       "3           1.5          0.2  setosa\n",
       "4           1.4          0.2  setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_pickle('toy_dataset.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digamos que la tarea en cuestión es predecir cual sera el largo del petalo (Y) sabiendo cual es el ancho del petalo (X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División estandar train-val-test\n",
    "   - Suficiente cantidad de datos\n",
    "   - El dev set lo puedo manejar como mas me convenga\n",
    "\n",
    " para este primer ejemplo, vamos a tomar 80% del dataset como dev set, y 20% como test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = data.sample(frac=0.8)\n",
    "data_test = data.drop(data_dev.index)\n",
    "data_train = data_dev.sample(frac=0.8)\n",
    "data_val = data_dev.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "![Illustration of the k-fold cross-validation procedure.](images/k_fold_cross_validation.png)\n",
    "\n",
    "   - kfoldCV, LOOCV\n",
    "   - un modelo por cada fold\n",
    "   - NO SE PROMEDIA EL RENDIMIENTO EN CADA FOLD!\n",
    "   - el mejor modelo se entrena en todo DEV\n",
    "       - Significancia estadística\n",
    "   - se testea el mejor modelo entrenado en el mayor set de datos\n",
    "   - el problema suele ser que el set de testeo sigue siendo pequeño\n",
    "       - nested crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Ejercicio [k-fold]\n",
    "Definir cual es la mejor manera de formar folds en este caso (tamaño, selección), e implementar el bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "- Suele estar mas asociado a los cálculos de significancia estadística (IC)\n",
    "- Distintos val sets\n",
    "- Controlamos la cantidad de bootstrapping (puedo hacer esto con CV?)\n",
    "![Illustration of training and test data splits in the Leave-One-Out Bootstrap (LOOB).](images/LOOB.png)\n",
    "\n",
    "    $$\\frac{1}{k}\\sum_{k=1}^{K}\\frac{1}{B^{-k}}\\sum_{b \\in B^{-k}}L(y_{i}, \\hat{f}_{b}(x_{i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "- Para todo pre-procesamiento que requiera calcular estadísticos, estos deben calcularse sobre los datos de entrenamiento solamente. Ejemplos: normalización, selección de features, etc.\n",
    "- Una buena practica consiste en diseñar el código para que las funciones de entrenamiento solo vean los datos de entrenamiento.\n",
    "- Si usamos CV o bootstrapping, eso implica que las estadísticas cambian para cada modelo entrenado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias - Variance tradeoff\n",
    "- Modelos simples muestran alto bias y poca varianza.\n",
    "- Modelos complejos muestran bajo bias y mucha varianza.\n",
    "- La varianza la puedo evaluar como la diferencia de performance entre dev y test sets.\n",
    "- El bias lo puedo evaluar como la cantidad de error sobre el dev set. \n",
    "- Existen metodos para encontrar el balance adecuado para la complejidad del modelo: regularización, boosting y bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como va a comportarse el modelo en la práctica?\n",
    "   - Probablemente levemente mejor que lo que calculamos para en los datos de evaluación\n",
    "   - Nuestro rendimiento estimado tiene un sesgo pesimista por estar entrenado en menos datos\n",
    "       - Podemos estimar cuan pesimista es nuestra estimación del rendimiento haciendo curvas de rendimiento vs tamaño del set de entrenamiento (learning curves)\n",
    "        \n",
    "![image.png](attachment:image.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
