{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Tutorial 8: Introducción a  <img src=\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/images/pytorch.svg\" alt=\"image\" style=\"max-width: 3%; height: auto;\"> PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es <img src=\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/images/pytorch.svg\" alt=\"image\" style=\"max-width: 3%; height: auto;\"> PyTorch? \n",
    "\n",
    "PyTorch es una biblioteca de código abierto para el aprendizaje automático desarrollada por Facebook AI Research que permite crear redes neuronales de manera sencilla y flexible. Esta herramienta se puede utilizar tanto en CPUs como en GPUs, lo que facilita el desarrollo y la implementación de modelos de aprendizaje profundo.\n",
    "\n",
    "## ¿Por qué elegir <img src=\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/images/pytorch.svg\" alt=\"image\" style=\"max-width: 3%; height: auto;\"> PyTorch?\n",
    "\n",
    "A pesar de que existen otros frameworks como TensorFlow, PyTorch se ha convertido en una opción preferida para muchos investigadores y desarrolladores. A continuación, se presentan algunas de las razones por las cuales se elige PyTorch:\n",
    "\n",
    "1. ⭐ **Facilidad de uso**: \n",
    "   - La creación de redes neuronales en PyTorch es más \"Pythonic\", lo que significa que se asemeja más a la forma en que escribimos código en Python. Esto hace que sea más accesible para quienes están familiarizados con este lenguaje.\n",
    "   - La API de PyTorch es intuitiva, lo que facilita la comprensión y el uso de sus funciones.\n",
    "\n",
    "2. ⭐ **Mejor capacidad de depuración**: \n",
    "   - PyTorch utiliza un grafo computacional dinámico, lo que permite realizar cambios sobre la marcha. Esto contrasta con TensorFlow, que utiliza un grafo estático. Esta flexibilidad, junto con la sintaxis más natural de PyTorch, hace que la depuración sea más sencilla y eficiente.\n",
    "\n",
    "3. ⭐ **Popularidad creciente**: \n",
    "   - En los últimos años, PyTorch ha ganado mucha popularidad en la comunidad de aprendizaje automático. Es el framework predominante en muchos artículos de investigación y en bibliotecas como Hugging Face, lo que indica su amplia aceptación y uso en el campo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar la última versión de PyTorch, ver el [este instructivo](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [PyTorch Tensors](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los tensores son una estructura de datos especializada, muy similar a los arrays y matrices. \n",
    "- En PyTorch, usamos tensores para codificar las entradas y salidas de un modelo, así como los parámetros del mismo.\n",
    "- Los tensores son parecidos a los ndarrays de NumPy, pero con la ventaja de que <span style=\"color: purple;\">pueden ejecutarse en GPUs</span> u otro hardware especializado para acelerar los cálculos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización de Tensors\n",
    "\n",
    "Los tensores se pueden inicializar de varias maneras. Por ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Directamente desde datos**\n",
    "\n",
    "Los tensores se pueden crear directamente a partir de datos. El tipo de dato se infiere automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Desde un array de NumPy**\n",
    "\n",
    "Los tensores se pueden crear a partir de arrays de NumPy (y viceversa, mirá el puente con NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Desde otro Tensor**\n",
    "\n",
    "El nuevo Tensor mantiene las propiedades (forma, tipo de dato) del Tensor original, a menos que se sobrescriban explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7132, 0.7811],\n",
      "        [0.0986, 0.5075]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Con valores aleatorios o constantes**\n",
    "\n",
    "Aleatorios con `rand`, constantes a partir de `ones` o `zeros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.7571, 0.7462, 0.0434],\n",
      "        [0.7861, 0.4422, 0.3363]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos del Tensor\n",
    "\n",
    "Los atributos del tensor describen su forma, tipo de dato y el dispositivo en el que están almacenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones con Tensores\n",
    "\n",
    "- Existen [más de 100 operaciones](https://pytorch.org/docs/stable/torch.html) con tensores.\n",
    "- Incluyen transposición, indexación, slicing, operaciones matemáticas, álgebra lineal, muestreo aleatorio y más.\n",
    "- Cada una de estas operaciones <span style=\"color: purple;\">se puede ejecutar en la GPU</span> (generalmente a velocidades más altas que en una CPU). \n",
    "    - Si usas Google Colab, asigná una GPU yendo a Edición > Configuración del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Indexación y slicing al estilo de NumPy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Uniendo tensores**\n",
    "\n",
    "- Podemos usar `torch.cat` para concatenar una secuencia de tensores a lo largo de una dimensión dada. \n",
    "- Esto significa que los tensores se apilan uno al lado del otro en la misma dimensión, manteniendo las demás dimensiones igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- También podemos usar `torch.stack`, otra operación para unir tensores que es sutilmente diferente: apila los tensores a lo largo de una nueva dimensión. \n",
    "- Esto crea un nuevo tensor en el que los tensores originales se organizan en una nueva dimensión, aumentando así la cantidad de dimensiones del tensor resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.stack([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Multiplicando tensores**\n",
    "\n",
    "Multiplicación elemento a elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicación matricial entre dos tensores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Operaciones in-place**\n",
    "\n",
    "Las operaciones que tienen un sufijo _ son in-place. \n",
    "\n",
    "- Esto quiere decir que <span style=\"color: purple;\">modifica directamente el contenido del objeto o tensor en el que se está aplicando</span>, en lugar de crear una copia nueva con el resultado de la operación.\n",
    "- Por ejemplo: `x.copy_(y)` o `x.t_()` cambiarán el valor de x.\n",
    "- Las operaciones in-place ahorran algo de memoria, pero pueden ser problemáticas al calcular derivadas debido a la pérdida inmediata de historial. Por eso, su uso no se recomienda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relación con NumPy\n",
    "\n",
    "- Los Tensores en la CPU y los arrays de NumPy pueden compartir sus ubicaciones de memoria subyacentes, y cambiar uno afectará al otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">Tensor a NumPy Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Veamos como un cambio en el Tensor se refleja en el array de NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: purple;\">NumPy Array a Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los cambios en el array de NumPy se reflejan en el Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Datasets y DataLoaders](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El código para procesar nuestras muestras de datos puede volverse desordenado y difícil de mantener rápidamente.\n",
    "- Idealmente, queremos que nuestro código sobre el conjunto de datos esté desacoplado del código de entrenamiento del modelo para mejorar la legibilidad y la modularidad.\n",
    "- PyTorch proporciona dos primitivas de datos: `torch.utils.data.DataLoader` y `torch.utils.data.Dataset` que permiten usar datasets pre-cargados o propios.\n",
    "    - `Dataset`: almacena las muestras y sus etiquetas correspondientes.\n",
    "    - `DataLoader`: envuelve un iterable alrededor del `Dataset` para facilitar el acceso a las muestras. Es fundamental para:\n",
    "        - <span style=\"color: purple;\">Manejar conjuntos de datos grandes</span> (no siempre se pueden cargar en memoria de una, este permite cargar los datos de manera paulatina en batches)\n",
    "        - <span style=\"color: purple;\">Cargar datos</span> de forma eficiente <span style=\"color: purple;\">en minibatches</span> (automatiza este proceso de crear y gestionar los minibatches).\n",
    "        - Asegurar que los <span style=\"color: purple;\">datos se procesen en paralelo</span> para optimizar el tiempo de entrenamiento (cargar datos puede ser un bottleneck si el proceso de carga es lento).\n",
    "        - Aplicar <span style=\"color: purple;\">transformaciones automáticamente</span> (como normalización, recorte de imágenes, etc.) a medida que se cargan los datos.\n",
    "        - <span style=\"color: purple;\">Facilitar el uso de GPUs</span> (transferiere los datos desde la CPU a la GPU).\n",
    "- Las bibliotecas en PyTorch ofrecen datasets integrados de alta calidad que podés usar en `Dataset`. Estos están disponibles actualmente en:\n",
    "    - [torchvision](https://pytorch.org/vision/stable/index.html)\n",
    "    - [torchaudio](https://pytorch.org/audio/stable/index.html)\n",
    "    - [torchtext](https://pytorch.org/text/stable/index.html)\n",
    "        - [Tutorial](https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html) de como cargar la data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando un Dataset\n",
    "\n",
    "Ejemplo: Cargar el dataset [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) desde TorchVision. \n",
    "\n",
    "<img src=\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/images/fashion-mnist-sprite.png\" alt=\"image\" style=\"display: block; margin: 0 auto; max-width: 50%; height: auto;\">\n",
    "\n",
    "\n",
    "- Fashion-MNIST es un conjunto de datos de imágenes de artículos de Zalando que consta de 60,000 ejemplos de entrenamiento y 10,000 ejemplos de prueba. \n",
    "- Cada ejemplo incluye una <span style=\"color: purple;\">imagen en escala de grises de 28×28</span> y una etiqueta asociada de una de <span style=\"color: red;\">10 clases</span>.\n",
    "\n",
    "Cargamos el conjunto de datos FashionMNIST con los siguientes parámetros:\n",
    "- `root` es la ruta donde se almacenan los datos de entrenamiento/prueba.\n",
    "- `train` especifica si es un conjunto de datos de entrenamiento o de prueba.\n",
    "- `download=True` descarga los datos de internet si no están disponibles en `root`.\n",
    "- `transform` y `target_transform` especifican las transformaciones de características y etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterando y Visualizando el Dataset\n",
    "\n",
    "- Podemos indexar manualmente los Datasets como si fueran una lista: `training_data[index]`. \n",
    "- Usamos matplotlib para visualizar algunas muestras de nuestros datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando un Dataset Personalizado para tus archivos\n",
    "\n",
    "- Una clase de Dataset personalizada debe implementar tres funciones: `__init__`, `__len__` y `__getitem__`. \n",
    "- En la siguiente implementación, las imágenes de FashionMNIST se almacenan en un directorio `img_dir`, y sus etiquetas se guardan por separado en un archivo CSV `annotations_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desglosemos qué ocurre en cada una de las funciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `__init__`\n",
    "\n",
    "- Se ejecuta una vez cuando se instancia el objeto `Dataset`. \n",
    "- Inicializamos el directorio que contiene las imágenes, el archivo de labels y las transformaciones que pueden tener.\n",
    "\n",
    "    El archivo `labels.csv` se ve así:\n",
    "\n",
    "    >tshirt1.jpg, 0\n",
    "    >\n",
    "    >tshirt2.jpg, 0\n",
    "    >\n",
    "    >......\n",
    "    >\n",
    "    >ankleboot999.jpg, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `__len__`\n",
    "- Devuelve la cantidad de muestras en nuestro conjunto de datos. Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `__getitem__`\n",
    "\n",
    "- Carga y devuelve una muestra del dataset en el índice dado `idx`. \n",
    "- Basándose en el índice:\n",
    "    1. Identifica la ubicación de la imagen en el disco\n",
    "    2. La convierte en un tensor usando `read_image`\n",
    "    3. Recupera la etiqueta correspondiente del archivo CSV en `self.img_labels`\n",
    "    4. Aplica las funciones de transformación (si corresponde) \n",
    "    5. Devuelve la imagen en formato Tensor y la etiqueta correspondiente en una tupla.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando los datos para el entrenamiento con DataLoaders\n",
    "\n",
    "- El Dataset recupera las features y labels de nuestro dataset de a una muestra. \n",
    "- Durante el entrenamiento de un modelo, generalmente queremos pasar muestras en “minibatches”, reshuffle los datos en cada época para reducir el overfitting del modelo, y usar multiprocesamiento en Python para acelerar la recuperación de datos.\n",
    "- `DataLoader` es un iterable que abstrae esta complejidad y nos proporciona una API sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterar a través del DataLoader\n",
    "\n",
    "- Ya cargamos el dataset en el `DataLoader` y podemos iterar sobre el dataset según sea necesario. \n",
    "- Cada iteración a continuación devuelve un batch de `train_features` y `train_labels` (que contienen `batch_size=64` features y labels respectivamente). \n",
    "- Como especificamos `shuffle=True`, después de iterar sobre todos los lotes, los datos se mezclan. \n",
    "- Para un control más detallado sobre el orden de carga de los datos, ver [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader)) # Obtiene un batch del dataloader para el entrenamiento\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimensión del batch `torch.Size([64, 1, 28, 28])`, se puede desglosar de la siguiente manera:\n",
    "\n",
    "- **64**: Es el tamaño del batch, lo que significa que en este caso estamos procesando 64 imágenes a la vez.\n",
    "- **1**: Representa la cantidad de canales. En este caso, las imágenes son en escala de grises (1 canal). Si fueran a color, el valor sería 3 para los canales RGB.\n",
    "- **28**: Es la altura de cada imagen.\n",
    "- **28**: Es el ancho de cada imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones\n",
    "\n",
    "- Los datos no siempre vienen en su forma final procesada que se requiere para entrenar algoritmos de machine learning. \n",
    "- Usamos transformaciones para realizar algunas manipulaciones de los datos y hacerlos adecuados para el entrenamiento.\n",
    "- Todos los datasets de TorchVision tienen dos parámetros: \n",
    "    - `transform` para modificar las características \n",
    "    - `target_transform` para modificar las etiquetas\n",
    "\n",
    "- El módulo `torchvision.transforms` ofrece varias transformaciones comunes listas para usar.\n",
    "\n",
    "- Las features de FashionMNIST están en formato de imagen PIL, y las etiquetas son números enteros. \n",
    "    - Para el entrenamiento, necesitamos que las <span style=\"color: purple;\">features sean tensores normalizados</span> y las <span style=\"color: purple;\">etiquetas sean tensores codificados en one-hot</span>. \n",
    "    - Para realizar estas transformaciones, usamos `ToTensor` y `Lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToTensor()\n",
    "\n",
    "- `ToTensor()` convierte una imagen en formato PIL o un arreglo NumPy (`ndarray`) en un `FloatTensor`.\n",
    "- Además, escala los valores de intensidad de los píxeles de la imagen al rango [0, 1], donde los píxeles originalmente tienen valores entre 0 y 255. \n",
    "- Es útil para normalizar las imágenes antes de entrenar modelos de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Transforms\n",
    "\n",
    "- Las transformaciones `Lambda` aplican cualquier función definida por el usuario mediante una función lambda. \n",
    "- Ejemplo: definimos una función para <span style=\"color: purple;\">convertir un entero en un tensor codificado en one-hot</span>. \n",
    "\n",
    "    1. Crea un tensor de ceros de tamaño 10 (la cantidad de etiquetas en nuestro conjunto de datos) \n",
    "    2. Llama a `scatter_`, que asigna el valor `1` en el índice correspondiente a la etiqueta `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, \n",
    "                                                                                index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de la Red Neuronal\n",
    "\n",
    "- Las redes neuronales están compuestas por capas/módulos que realizan operaciones sobre los datos. \n",
    "- [torch.nn](https://pytorch.org/docs/stable/nn.html) proporciona todos los bloques de construcción necesarios para armar nuestra propia red neuronal. \n",
    "- Cada módulo en PyTorch es una subclase de [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). \n",
    "- <span style=\"color: purple;\">Una red neuronal es un módulo en sí mismo que consiste en otros módulos (las capas)</span>. \n",
    "- Esta estructura anidada permite construir y gestionar arquitecturas complejas de manera sencilla.\n",
    "\n",
    "A continuación vamos a construir una red neuronal para clasificar imágenes del dataset FashionMNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispositivo para Entrenamiento\n",
    "\n",
    "Si queremos entrenar nuestro modelo en un acelerador de hardware como la GPU (Graphics Processor Unit) o MPS (Metal Performance Shaders) tenemos que verificar si [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) o [torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) están disponibles. Si no lo están, usamos la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la Clase\n",
    "\n",
    "- Definimos nuestra red neuronal subclasando `nn.Module`\n",
    "- Inicializamos las capas de la red neuronal en `__init__`. \n",
    "- Cada subclase de `nn.Module` implementa las operaciones sobre los datos de entrada en el método `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una instancia de `NeuralNetwork`, la movemos al dispositivo correspondiente (GPU o CPU), y mostramos su estructura con un `print`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para usar el modelo, le tenemos que pasar los datos de entrada. \n",
    "- Esto va a ejecutar el método `forward` del modelo, junto con [algunas operaciones](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866) en segundo plano. \n",
    "    - No llamar directamente a `model.forward()`!\n",
    "        - Al hacer esto, PyTorch también maneja el seguimiento del gradiente y otras operaciones necesarias para el entrenamiento, como la actualización de los parámetros del modelo. Si llamamos a `model.forward()` directamente, podemos omitir estas funciones automáticas, lo que podría llevar a errores o un comportamiento inesperado en el modelo.\n",
    "\n",
    "- Al llamar al modelo con la entrada, obtenemos un tensor de 2 dimensiones:\n",
    "    - `dim=0`: las 10 predicciones crudas para cada clase \n",
    "    - `dim=1`: valores individuales de cada salida. \n",
    "    \n",
    "- Obtenemos las probabilidades de predicción al pasar el resultado a través de una instancia del módulo `nn.Softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capas del Modelo\n",
    "\n",
    "Vamos a desglosar las capas de nuestro modelo de FashionMNIST. \n",
    "\n",
    "- Para ilustrarlo, vamos a tomar un minibatch de 3 imágenes de tamaño 28x28.\n",
    "- Veamos qué sucede al pasarlas a través de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
    "\n",
    "- Inicializamos la capa `nn.Flatten` para convertir cada imagen 2D de 28x28 en un array contiguo de 784 valores de píxeles (se mantiene la dimensión del minibatch en `dim=0`). \n",
    "- Esto permite que la red neuronal procese las imágenes en forma de un vector unidimensional, lo que es necesario para las capas densas que seguirán.\n",
    "\n",
    "<img src=\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/images/flatten.png\" alt=\"image\" style=\"display: block; margin: 0 auto; max-width: 50%; height: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "- La capa lineal es un módulo que aplica una transformación lineal a la entrada utilizando sus pesos y biases almacenados. \n",
    "\n",
    "$$ y = xW^T+b$$\n",
    "\n",
    "- Toma el vector de entrada (en este caso, el vector de 784 elementos) y lo multiplica por una matriz de pesos, además de sumar un vector de sesgos, produciendo una salida que puede ser pasada a la siguiente capa en la red neuronal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "\n",
    "- Las activaciones no lineales son las que crean los mapeos complejos entre las entradas y salidas del modelo. \n",
    "- Se aplican después de las transformaciones lineales para introducir no linealidad, lo que ayuda a las redes neuronales a aprender una amplia variedad de fenómenos.\n",
    "- En este modelo, usamos `nn.ReLU` entre nuestras capas lineales, pero hay otras funciones de activación que también puedes usar para introducir no linealidad en tu modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.2496, -0.0478, -0.4670,  0.6656, -0.1629,  0.1780, -0.3483, -0.3374,\n",
      "         -0.1649,  0.3486,  0.2182,  0.0984,  0.1837, -0.2095,  0.1480,  0.0752,\n",
      "         -0.0132, -0.1208, -0.2311,  0.2011],\n",
      "        [-0.1118, -0.0152, -0.6793,  0.3207, -0.1928,  0.1580, -0.3974, -0.2909,\n",
      "         -0.1026,  0.3794,  0.0496, -0.0631,  0.0679,  0.1760,  0.1177,  0.0898,\n",
      "         -0.0630, -0.2586, -0.0552,  0.4118],\n",
      "        [ 0.0035, -0.3443, -0.1378,  0.0421, -0.1592,  0.2277, -0.3902, -0.1950,\n",
      "         -0.1014,  0.4022, -0.1550, -0.2671,  0.2051, -0.1180,  0.2900, -0.0054,\n",
      "          0.1175, -0.2513,  0.0041,  0.2503]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.2496, 0.0000, 0.0000, 0.6656, 0.0000, 0.1780, 0.0000, 0.0000, 0.0000,\n",
      "         0.3486, 0.2182, 0.0984, 0.1837, 0.0000, 0.1480, 0.0752, 0.0000, 0.0000,\n",
      "         0.0000, 0.2011],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3207, 0.0000, 0.1580, 0.0000, 0.0000, 0.0000,\n",
      "         0.3794, 0.0496, 0.0000, 0.0679, 0.1760, 0.1177, 0.0898, 0.0000, 0.0000,\n",
      "         0.0000, 0.4118],\n",
      "        [0.0035, 0.0000, 0.0000, 0.0421, 0.0000, 0.2277, 0.0000, 0.0000, 0.0000,\n",
      "         0.4022, 0.0000, 0.0000, 0.2051, 0.0000, 0.2900, 0.0000, 0.1175, 0.0000,\n",
      "         0.0041, 0.2503]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "\n",
    "- Es un contenedor ordenado de módulos. \n",
    "- Los datos se pasan a través de todos los módulos en el mismo orden en que están definidos. \n",
    "- Simplifica la creación de redes neuronales al permitirte apilar capas de manera sencilla y clara, sin tener que definir manualmente el flujo de datos en el método `forward`. \n",
    "- Es útil para modelos más simples donde el flujo es lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
    "\n",
    "- La última capa lineal de la red neuronal devuelve logits.\n",
    "    - Logits: valores crudos en el rango $[-\\infty, \\infty]$. \n",
    "- Estos logits se pasan al módulo `nn.Softmax`. \n",
    "    - La función `Softmax` escala los logits a valores en el rango $[0, 1]$, representando las probabilidades predichas por el modelo para cada clase.\n",
    "\n",
    "<img src=\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/images/logits.jpg\" alt=\"image\" style=\"display: block; margin: 0 auto; max-width: 50%; height: auto;\">\n",
    "\n",
    "El parámetro `dim` indica la dimensión a lo largo de la cual los valores deben sumar 1, asegurando que las probabilidades de todas las clases en la salida se normalicen correctamente. Esto es crucial para tareas de clasificación, ya que permite interpretar las salidas del modelo como probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example for image 1:\n",
      "\n",
      "Logits:\n",
      " tensor([-0.1815, -0.0684, -0.0429, -0.1391, -0.0150, -0.1737,  0.1726, -0.1623,\n",
      "         0.0643,  0.1264], grad_fn=<SelectBackward0>)\n",
      "Probabilities:\n",
      " tensor([0.0863, 0.0967, 0.0992, 0.0901, 0.1020, 0.0870, 0.1230, 0.0880, 0.1104,\n",
      "        0.1174], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Sum of probabilities (after Softmax): 1\n",
      "Class predicted: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Example for image 1:\\n\")\n",
    "print(f\"Logits:\\n {logits[0]}\")\n",
    "print(f\"Probabilities:\\n {pred_probab[0]}\\n\")\n",
    "print(f\"Sum of probabilities (after Softmax): {int(pred_probab[0].sum())}\")\n",
    "print(f\"Class predicted: {pred_probab[0].argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros del Modelo\n",
    "\n",
    "- Como sabemos, muchas capas dentro de una red neuronal son paramétricas (tienen pesos y sesgos asociados que se optimizan durante el entrenamiento). \n",
    "- Al subclasear `nn.Module`, se rastrean automáticamente todos los campos definidos dentro del objeto de modelo, y <span style=\"color: purple;\">todos los parámetros son accesibles</span> mediante los métodos `parameters()` o `named_parameters()` de tu modelo.\n",
    "\n",
    "- En este ejemplo, iteramos sobre cada parámetro y mostramos su tamaño y una vista previa de sus valores.\n",
    "- Esto es útil para entender la estructura del modelo y verificar que los parámetros se están inicializando correctamente antes del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0062,  0.0269,  0.0141,  ...,  0.0212,  0.0104, -0.0164],\n",
      "        [-0.0134, -0.0065, -0.0240,  ...,  0.0266,  0.0140,  0.0132]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0100,  0.0005], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0036,  0.0306, -0.0198,  ...,  0.0248,  0.0258,  0.0037],\n",
      "        [ 0.0374,  0.0229, -0.0038,  ...,  0.0122, -0.0121,  0.0106]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0024, -0.0175], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0032,  0.0281,  0.0049,  ..., -0.0410, -0.0211, -0.0058],\n",
      "        [-0.0300, -0.0360, -0.0009,  ...,  0.0435,  0.0368,  0.0019]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0019, -0.0415], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferenciación Automática con `torch.autograd`\n",
    "\n",
    "- Al entrenar redes neuronales, el algoritmo más utilizado es la retropropagación. \n",
    "- En este algoritmo, se ajustan los parámetros (pesos del modelo) de acuerdo con el gradiente de la función de pérdida con respecto al parámetro dado.\n",
    "- Para calcular esos gradientes, PyTorch tiene un motor de diferenciación incorporado llamado `torch.autograd`. Este motor __soporta el cálculo automático del gradiente para cualquier grafo computacional__.\n",
    "\n",
    "Consideremos la red neuronal más simple, que tiene una sola capa con entrada $x$, parámetros $w$ y $b$, y una función de pérdida. Esto se puede definir en PyTorch de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores, Funciones y Grafo Computacional\n",
    "\n",
    "Este código define el siguiente grafo computacional:\n",
    "\n",
    "<img src=\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/images/comp-graph.png\" alt=\"image\" style=\"display: block; margin: 0 auto; max-width: 50%; height: auto;\">\n",
    "\n",
    "- En esta red, $w$ y $b$ son parámetros que necesitamos optimizar. \n",
    "- Por lo tanto, debemos poder calcular los gradientes de la función de pérdida con respecto a esas variables. \n",
    "- Para hacer esto, establecemos la propiedad `requires_grad` de esos tensores.\n",
    "    - Al configurar `requires_grad=True`, indicamos a PyTorch que queremos calcular y almacenar los gradientes de estos tensores durante el proceso de backpropagation. \n",
    "    - Esto es esencial para actualizar los parámetros del modelo utilizando algoritmos de optimización como el descenso de gradiente. \n",
    "    - Así, cada vez que realicemos una operación con estos tensores, PyTorch construirá el grafo computacional necesario para calcular los gradientes automáticamente.\n",
    "- NOTA: Podés establecer el valor de `requires_grad` al crear un tensor, o hacerlo más tarde usando el método `x.requires_grad_(True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una función que aplicamos a tensores para construir el grafo computacional es, de hecho, un objeto de la clase `Function`. \n",
    "- Este objeto sabe cómo calcular la función en la dirección hacia adelante y también cómo calcular su derivada durante el paso de retropropagación. \n",
    "- Una referencia a la función de backpropagation se almacena en la propiedad `grad_fn` de un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f5c476b17c0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f5c476b1220>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computando Gradientes  \n",
    "- Para optimizar los pesos de los parámetros en la red neuronal, necesitamos calcular las derivadas de nuestra función de pérdida con respecto a los parámetros\n",
    "    - Es decir, necesitamos $\\frac{\\partial \\text{loss}}{\\partial w}$ y $\\frac{\\partial \\text{loss}}{\\partial b}$ bajo algunos valores fijos de $x$ e $y$. \n",
    "- Para calcular esas derivadas, llamamos a `loss.backward()`, y luego recuperamos los valores de `w.grad` y `b.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0983, 0.0479, 0.1606],\n",
      "        [0.0983, 0.0479, 0.1606],\n",
      "        [0.0983, 0.0479, 0.1606],\n",
      "        [0.0983, 0.0479, 0.1606],\n",
      "        [0.0983, 0.0479, 0.1606]])\n",
      "tensor([0.0983, 0.0479, 0.1606])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: Solo los nodos que son considerados \"hoja\" (es decir, aquellos que no tienen otros nodos como dependencias) y que tienen la propiedad `requires_grad` activada pueden proporcionar información sobre sus gradientes. Para todos los demás nodos en nuestro grafo, los gradientes no estarán disponibles.\n",
    "\n",
    "Además, cuando llamamos a `backward()` para calcular los gradientes, solo podemos hacerlo una vez por cada grafo. Si queremos calcular los gradientes varias veces en el mismo grafo, debemos usar el parámetro `retain_graph=True`, lo que permite que el grafo se mantenga en memoria para cálculos futuros. Esto es útil para evitar que se pierda la información del grafo después de una primera pasada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desactivando el seguimiento de gradientes  \n",
    "\n",
    "- Por defecto, los tensores que tienen `requires_grad=True` guardan un registro de cómo se calcularon para poder hacer cálculos de gradientes. \n",
    "- Pero a veces, como cuando ya entrenamos el modelo y solo __queremos usarlo para hacer predicciones__, no necesitamos ese seguimiento. \n",
    "- Para evitarlo y hacer las cosas más rápidas, podemos usar un bloque `torch.no_grad()`, que desactiva temporalmente ese seguimiento mientras hacemos los cálculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de lograr el mismo resultado es usar el método `detach()` en el tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤔 ¿Por qué razones se les ocurre que podríamos querer desactivar el seguimiento de gradientes?\n",
    "\n",
    "1. **Congelar parámetros**: Si queremos marcar algunos parámetros en nuestra red neuronal como parámetros congelados (es decir, que no se actualizarán durante el entrenamiento).\n",
    "\n",
    "2. **Acelerar cálculos**: Cuando solo estamos haciendo forward propagation, las operaciones en tensores que no rastrean gradientes son más eficientes, lo que puede hacer que nuestros cálculos sean más rápidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más sobre gráficos computacionales\n",
    "\n",
    "`autograd` es una herramienta en PyTorch que nos ayuda a calcular gradientes automáticamente. Imaginemos que todos nuestros datos (tensores) y las operaciones que hacemos con ellos forman un gráfico (grafo dirigido acíclico, DAG). Este gráfico tiene dos tipos de nodos: \n",
    "\n",
    "- **Hojas**: son nuestros datos de entrada.\n",
    "- **Raíces**: son los resultados que obtenemos.\n",
    "\n",
    "Cuando hacemos cálculos, `autograd` registra todo en este grafo. Así, podemos calcular los gradientes cuando los necesitemos.\n",
    "\n",
    "**Pasos de funcionamiento:**\n",
    "\n",
    "1. **Paso hacia adelante**: Cuando calculamos algo, `autograd` realiza la operación y guarda información sobre cómo hacerlo de nuevo si es necesario.\n",
    "   \n",
    "2. **Paso hacia atrás**: Cuando llamamos a `.backward()`, `autograd`:\n",
    "   - Calcula los gradientes de todos los pasos que realizamos.\n",
    "   - Guarda esos gradientes en los tensores correspondientes.\n",
    "   - Propaga esos gradientes hacia los datos de entrada.\n",
    "\n",
    "Esto nos permite optimizar los parámetros de nuestro modelo, ajustando los pesos para mejorar su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizando los Parámetros del Modelo\n",
    "\n",
    "Ahora que tenemos un modelo y datos, es momento de entrenar, validar y probar nuestro modelo optimizando sus parámetros. El entrenamiento de un modelo es un proceso iterativo. En cada iteración tenemos:\n",
    "\n",
    "1. **Predicción**: El modelo hace una estimación sobre el resultado.\n",
    "2. **Cálculo de Error**: Se calcula el error de esa estimación (pérdida).\n",
    "3. **Derivadas**: Se recolectan las derivadas del error con respecto a los parámetros del modelo.\n",
    "4. **Optimización**: Se optimizan estos parámetros utilizando el descenso de gradiente.\n",
    "\n",
    "Este proceso se repite varias veces hasta que el modelo mejora y se ajusta mejor a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros\n",
    "\n",
    "- Recordemos que los hiperparámetros son parámetros ajustables que nos permiten controlar el proceso de optimización del modelo. Por ejemplo:\n",
    "\n",
    "1. **Épocas**: Cuántas veces se itera sobre el conjunto de datos.\n",
    "\n",
    "2. **Tamaño del Lote (Batch Size)**: La cantidad de muestras de datos que se propagan a través de la red antes de que se actualicen los parámetros.\n",
    "\n",
    "3. **Tasa de Aprendizaje (Learning Rate)**: Cuánto se actualizan los parámetros del modelo en cada lote/época. \n",
    "    - Valores más pequeños resultan en una velocidad de aprendizaje lenta, mientras que valores grandes pueden llevar a comportamientos impredecibles durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucle de Optimización\n",
    "\n",
    "- Una vez que establecemos nuestros hiperparámetros, podemos entrenar y optimizar nuestro modelo con un bucle de optimización.\n",
    "\n",
    "- Cada época (1 iteración del bucle) consta de dos partes principales:\n",
    "\n",
    "    1. **Bucle de Entrenamiento**: Iterar sobre el set de entrenamiento y tratar de converger hacia parámetros óptimos.\n",
    "\n",
    "    2. **Bucle de Validación/Test**: Iterar sobre el set de prueba para verificar si el rendimiento del modelo está mejorando.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de Pérdida\n",
    "\n",
    "- Mide el grado de disimilitud entre el resultado obtenido y el valor objetivo. \n",
    "- Es esta función la que queremos minimizar durante el entrenamiento. \n",
    "- [Ver todas las loss functions de Pytorch](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Para nuestra red, vamos a pasar los logits de salida de nuestro modelo a [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), que normaliza los logits y calcula el error de predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizador\n",
    "\n",
    "- Toda la lógica de optimización está encapsulada en el objeto `optimizer`. \n",
    "- En este ejemplo utilizamos el optimizador SGD.\n",
    "- Hay muchos otros [optimizadores disponibles en PyTorch](https://pytorch.org/docs/stable/optim.html), como ADAM y RMSProp, que funcionan mejor para diferentes tipos de modelos y datos.\n",
    "- Inicializamos el optimizador registrando los parámetros del modelo que deben ser entrenados y pasando el hiperparámetro de la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del bucle de entrenamiento, la optimización ocurre en tres pasos:\n",
    "\n",
    "1. Llamar a `optimizer.zero_grad()` para __restablecer los gradientes de los parámetros__ del modelo. Por defecto, los gradientes se suman; para evitar contar dos veces, los reiniciamos en cada iteración.\n",
    "\n",
    "2. __Retropropagar la pérdida de la predicción__ con una llamada a `loss.backward()`. PyTorch guarda los gradientes de la pérdida con respecto a cada parámetro.\n",
    "\n",
    "3. Una vez que tenemos nuestros gradientes, llamamos a `optimizer.step()` para __ajustar los parámetros__ según los gradientes recogidos en la retropropagación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación Completa\n",
    "\n",
    "Definimos el `train_loop` que itera sobre nuestro código de optimización, y el `test_loop` que evalúa el rendimiento del modelo con respecto a nuestros datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)  # Obtener el tamaño del conjunto de datos\n",
    "    # Configura el modelo en modo de entrenamiento: importante para batch normalization y dropout\n",
    "    # Aunque no es necesario en esta situación, se incluye por buenas prácticas\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)  # Realiza la predicción usando el modelo\n",
    "        loss = loss_fn(pred, y)  # Calcula la pérdida comparando la predicción con las etiquetas verdaderas\n",
    "\n",
    "        # Retropropagación\n",
    "        loss.backward()  # Calcula los gradientes de la pérdida con respecto a los parámetros del modelo\n",
    "        optimizer.step()  # Actualiza los parámetros del modelo usando los gradientes\n",
    "        optimizer.zero_grad()  # Reinicia los gradientes para la siguiente iteración\n",
    "\n",
    "        # Imprimir cada 100 lotes\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)  # Obtener la pérdida y la posición actual\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")  # Imprimir la pérdida y el progreso\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Configura el modelo en modo de evaluación: importante para batch normalization y dropout\n",
    "    # Aunque no es necesario en esta situación, se incluye por buenas prácticas\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)  # Obtener el tamaño del conjunto de datos\n",
    "    num_batches = len(dataloader)  # Obtener el número de lotes\n",
    "    test_loss, correct = 0, 0  # Inicializar la pérdida de prueba y el contador de aciertos\n",
    "\n",
    "    # Evaluar el modelo sin calcular gradientes para reducir el uso de memoria\n",
    "    # Esto asegura que no se calculen gradientes durante el modo de prueba\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)  # Realiza la predicción usando el modelo\n",
    "            test_loss += loss_fn(pred, y).item()  # Sumar la pérdida de prueba\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # Contar aciertos comparando predicciones con etiquetas verdaderas\n",
    "\n",
    "    test_loss /= num_batches  # Calcular la pérdida promedio de prueba\n",
    "    correct /= size  # Calcular la precisión\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")  # Imprimir precisión y pérdida promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la función de pérdida y el optimizador, y los pasamos a `train_loop` y `test_loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.319250  [   64/60000]\n",
      "loss: 2.300443  [ 6464/60000]\n",
      "loss: 2.292936  [12864/60000]\n",
      "loss: 2.264117  [19264/60000]\n",
      "loss: 2.265318  [25664/60000]\n",
      "loss: 2.234317  [32064/60000]\n",
      "loss: 2.227516  [38464/60000]\n",
      "loss: 2.219721  [44864/60000]\n",
      "loss: 2.194657  [51264/60000]\n",
      "loss: 2.176153  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.7%, Avg loss: 2.176534 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.164318  [   64/60000]\n",
      "loss: 2.153412  [ 6464/60000]\n",
      "loss: 2.127674  [12864/60000]\n",
      "loss: 2.120450  [19264/60000]\n",
      "loss: 2.093173  [25664/60000]\n",
      "loss: 2.104707  [32064/60000]\n",
      "loss: 2.023111  [38464/60000]\n",
      "loss: 2.017423  [44864/60000]\n",
      "loss: 1.989772  [51264/60000]\n",
      "loss: 1.871518  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.919991 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.854432  [   64/60000]\n",
      "loss: 1.870534  [ 6464/60000]\n",
      "loss: 1.868607  [12864/60000]\n",
      "loss: 1.764770  [19264/60000]\n",
      "loss: 1.730715  [25664/60000]\n",
      "loss: 1.703118  [32064/60000]\n",
      "loss: 1.696072  [38464/60000]\n",
      "loss: 1.680326  [44864/60000]\n",
      "loss: 1.549958  [51264/60000]\n",
      "loss: 1.469431  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 1.553636 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.596448  [   64/60000]\n",
      "loss: 1.578996  [ 6464/60000]\n",
      "loss: 1.433099  [12864/60000]\n",
      "loss: 1.375206  [19264/60000]\n",
      "loss: 1.399597  [25664/60000]\n",
      "loss: 1.333312  [32064/60000]\n",
      "loss: 1.379811  [38464/60000]\n",
      "loss: 1.280146  [44864/60000]\n",
      "loss: 1.236352  [51264/60000]\n",
      "loss: 1.232275  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.283944 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.224371  [   64/60000]\n",
      "loss: 1.262550  [ 6464/60000]\n",
      "loss: 1.265911  [12864/60000]\n",
      "loss: 1.262347  [19264/60000]\n",
      "loss: 0.999346  [25664/60000]\n",
      "loss: 1.264044  [32064/60000]\n",
      "loss: 1.318160  [38464/60000]\n",
      "loss: 1.174422  [44864/60000]\n",
      "loss: 1.043823  [51264/60000]\n",
      "loss: 1.026406  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.114087 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.147906  [   64/60000]\n",
      "loss: 1.083606  [ 6464/60000]\n",
      "loss: 1.129345  [12864/60000]\n",
      "loss: 1.044624  [19264/60000]\n",
      "loss: 1.152343  [25664/60000]\n",
      "loss: 1.042800  [32064/60000]\n",
      "loss: 1.011050  [38464/60000]\n",
      "loss: 0.934707  [44864/60000]\n",
      "loss: 0.972649  [51264/60000]\n",
      "loss: 1.035328  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.002965 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.035045  [   64/60000]\n",
      "loss: 0.993661  [ 6464/60000]\n",
      "loss: 0.919828  [12864/60000]\n",
      "loss: 1.017292  [19264/60000]\n",
      "loss: 1.000797  [25664/60000]\n",
      "loss: 1.056395  [32064/60000]\n",
      "loss: 0.943105  [38464/60000]\n",
      "loss: 0.850750  [44864/60000]\n",
      "loss: 0.857815  [51264/60000]\n",
      "loss: 0.886651  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.927749 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.792986  [   64/60000]\n",
      "loss: 0.777665  [ 6464/60000]\n",
      "loss: 0.749361  [12864/60000]\n",
      "loss: 0.822893  [19264/60000]\n",
      "loss: 0.948349  [25664/60000]\n",
      "loss: 0.898705  [32064/60000]\n",
      "loss: 0.815374  [38464/60000]\n",
      "loss: 0.895906  [44864/60000]\n",
      "loss: 0.758852  [51264/60000]\n",
      "loss: 0.933321  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.873126 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.789247  [   64/60000]\n",
      "loss: 0.760153  [ 6464/60000]\n",
      "loss: 0.838601  [12864/60000]\n",
      "loss: 0.779514  [19264/60000]\n",
      "loss: 0.802371  [25664/60000]\n",
      "loss: 0.716930  [32064/60000]\n",
      "loss: 0.811260  [38464/60000]\n",
      "loss: 0.676920  [44864/60000]\n",
      "loss: 1.022219  [51264/60000]\n",
      "loss: 0.706177  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.832461 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.696326  [   64/60000]\n",
      "loss: 0.911656  [ 6464/60000]\n",
      "loss: 0.884726  [12864/60000]\n",
      "loss: 0.669639  [19264/60000]\n",
      "loss: 0.654570  [25664/60000]\n",
      "loss: 0.669501  [32064/60000]\n",
      "loss: 0.959034  [38464/60000]\n",
      "loss: 0.764418  [44864/60000]\n",
      "loss: 0.803761  [51264/60000]\n",
      "loss: 0.817050  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.800303 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.776202  [   64/60000]\n",
      "loss: 0.780817  [ 6464/60000]\n",
      "loss: 0.903987  [12864/60000]\n",
      "loss: 0.606332  [19264/60000]\n",
      "loss: 0.610962  [25664/60000]\n",
      "loss: 0.793215  [32064/60000]\n",
      "loss: 0.830170  [38464/60000]\n",
      "loss: 0.702211  [44864/60000]\n",
      "loss: 0.528575  [51264/60000]\n",
      "loss: 0.761897  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.772539 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.775776  [   64/60000]\n",
      "loss: 0.697797  [ 6464/60000]\n",
      "loss: 0.769434  [12864/60000]\n",
      "loss: 0.599173  [19264/60000]\n",
      "loss: 0.621940  [25664/60000]\n",
      "loss: 0.634735  [32064/60000]\n",
      "loss: 0.812587  [38464/60000]\n",
      "loss: 0.689814  [44864/60000]\n",
      "loss: 0.618877  [51264/60000]\n",
      "loss: 0.745652  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.750611 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.931191  [   64/60000]\n",
      "loss: 0.853576  [ 6464/60000]\n",
      "loss: 0.849963  [12864/60000]\n",
      "loss: 0.920631  [19264/60000]\n",
      "loss: 0.834074  [25664/60000]\n",
      "loss: 0.714145  [32064/60000]\n",
      "loss: 0.643364  [38464/60000]\n",
      "loss: 0.846785  [44864/60000]\n",
      "loss: 0.689954  [51264/60000]\n",
      "loss: 0.621518  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.729529 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.779796  [   64/60000]\n",
      "loss: 0.710660  [ 6464/60000]\n",
      "loss: 0.690441  [12864/60000]\n",
      "loss: 0.623888  [19264/60000]\n",
      "loss: 0.845417  [25664/60000]\n",
      "loss: 0.752527  [32064/60000]\n",
      "loss: 0.514463  [38464/60000]\n",
      "loss: 0.736343  [44864/60000]\n",
      "loss: 0.672035  [51264/60000]\n",
      "loss: 0.739389  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.710037 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.573345  [   64/60000]\n",
      "loss: 0.630194  [ 6464/60000]\n",
      "loss: 0.622280  [12864/60000]\n",
      "loss: 0.623805  [19264/60000]\n",
      "loss: 0.636789  [25664/60000]\n",
      "loss: 0.643218  [32064/60000]\n",
      "loss: 0.744707  [38464/60000]\n",
      "loss: 0.697214  [44864/60000]\n",
      "loss: 0.535751  [51264/60000]\n",
      "loss: 0.643751  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.693325 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.771112  [   64/60000]\n",
      "loss: 0.757494  [ 6464/60000]\n",
      "loss: 0.574018  [12864/60000]\n",
      "loss: 0.761909  [19264/60000]\n",
      "loss: 0.735726  [25664/60000]\n",
      "loss: 0.674907  [32064/60000]\n",
      "loss: 0.659164  [38464/60000]\n",
      "loss: 0.615607  [44864/60000]\n",
      "loss: 0.605291  [51264/60000]\n",
      "loss: 0.649106  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.678460 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.724784  [   64/60000]\n",
      "loss: 0.556634  [ 6464/60000]\n",
      "loss: 0.639524  [12864/60000]\n",
      "loss: 0.756801  [19264/60000]\n",
      "loss: 0.756672  [25664/60000]\n",
      "loss: 0.559389  [32064/60000]\n",
      "loss: 0.572372  [38464/60000]\n",
      "loss: 0.746455  [44864/60000]\n",
      "loss: 0.760185  [51264/60000]\n",
      "loss: 0.586373  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.665224 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.594133  [   64/60000]\n",
      "loss: 0.645892  [ 6464/60000]\n",
      "loss: 0.596118  [12864/60000]\n",
      "loss: 0.538835  [19264/60000]\n",
      "loss: 0.703213  [25664/60000]\n",
      "loss: 0.619630  [32064/60000]\n",
      "loss: 0.604453  [38464/60000]\n",
      "loss: 0.567222  [44864/60000]\n",
      "loss: 0.652002  [51264/60000]\n",
      "loss: 0.626722  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.651665 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.613159  [   64/60000]\n",
      "loss: 0.588408  [ 6464/60000]\n",
      "loss: 0.562073  [12864/60000]\n",
      "loss: 0.580523  [19264/60000]\n",
      "loss: 0.598845  [25664/60000]\n",
      "loss: 0.611222  [32064/60000]\n",
      "loss: 0.560468  [38464/60000]\n",
      "loss: 0.664566  [44864/60000]\n",
      "loss: 0.551522  [51264/60000]\n",
      "loss: 0.502201  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.638821 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.565336  [   64/60000]\n",
      "loss: 0.602193  [ 6464/60000]\n",
      "loss: 0.533942  [12864/60000]\n",
      "loss: 0.726935  [19264/60000]\n",
      "loss: 0.699107  [25664/60000]\n",
      "loss: 0.429990  [32064/60000]\n",
      "loss: 0.753417  [38464/60000]\n",
      "loss: 0.523048  [44864/60000]\n",
      "loss: 0.648786  [51264/60000]\n",
      "loss: 0.598027  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.629166 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.604927  [   64/60000]\n",
      "loss: 0.463880  [ 6464/60000]\n",
      "loss: 0.603579  [12864/60000]\n",
      "loss: 0.618919  [19264/60000]\n",
      "loss: 0.702225  [25664/60000]\n",
      "loss: 0.737364  [32064/60000]\n",
      "loss: 0.672996  [38464/60000]\n",
      "loss: 0.586596  [44864/60000]\n",
      "loss: 0.754685  [51264/60000]\n",
      "loss: 0.510732  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.617888 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.560072  [   64/60000]\n",
      "loss: 0.737810  [ 6464/60000]\n",
      "loss: 0.521477  [12864/60000]\n",
      "loss: 0.481501  [19264/60000]\n",
      "loss: 0.672277  [25664/60000]\n",
      "loss: 0.523033  [32064/60000]\n",
      "loss: 0.597968  [38464/60000]\n",
      "loss: 0.594362  [44864/60000]\n",
      "loss: 0.708022  [51264/60000]\n",
      "loss: 0.683714  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.610489 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.610398  [   64/60000]\n",
      "loss: 0.478852  [ 6464/60000]\n",
      "loss: 0.526410  [12864/60000]\n",
      "loss: 0.511690  [19264/60000]\n",
      "loss: 0.574761  [25664/60000]\n",
      "loss: 0.536545  [32064/60000]\n",
      "loss: 0.625258  [38464/60000]\n",
      "loss: 0.529866  [44864/60000]\n",
      "loss: 0.552383  [51264/60000]\n",
      "loss: 0.492560  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.598455 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.532223  [   64/60000]\n",
      "loss: 0.355079  [ 6464/60000]\n",
      "loss: 0.589137  [12864/60000]\n",
      "loss: 0.571991  [19264/60000]\n",
      "loss: 0.590366  [25664/60000]\n",
      "loss: 0.562543  [32064/60000]\n",
      "loss: 0.455685  [38464/60000]\n",
      "loss: 0.494030  [44864/60000]\n",
      "loss: 0.717718  [51264/60000]\n",
      "loss: 0.630198  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.591745 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.407781  [   64/60000]\n",
      "loss: 0.444893  [ 6464/60000]\n",
      "loss: 0.582946  [12864/60000]\n",
      "loss: 0.542648  [19264/60000]\n",
      "loss: 0.442905  [25664/60000]\n",
      "loss: 0.713353  [32064/60000]\n",
      "loss: 0.529282  [38464/60000]\n",
      "loss: 0.548431  [44864/60000]\n",
      "loss: 0.556552  [51264/60000]\n",
      "loss: 0.491357  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.584166 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.611386  [   64/60000]\n",
      "loss: 0.656839  [ 6464/60000]\n",
      "loss: 0.523798  [12864/60000]\n",
      "loss: 0.608150  [19264/60000]\n",
      "loss: 0.573354  [25664/60000]\n",
      "loss: 0.531954  [32064/60000]\n",
      "loss: 0.600898  [38464/60000]\n",
      "loss: 0.579093  [44864/60000]\n",
      "loss: 0.467659  [51264/60000]\n",
      "loss: 0.386394  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.577004 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.607247  [   64/60000]\n",
      "loss: 0.491680  [ 6464/60000]\n",
      "loss: 0.428038  [12864/60000]\n",
      "loss: 0.599879  [19264/60000]\n",
      "loss: 0.684440  [25664/60000]\n",
      "loss: 0.673039  [32064/60000]\n",
      "loss: 0.603023  [38464/60000]\n",
      "loss: 0.580076  [44864/60000]\n",
      "loss: 0.648630  [51264/60000]\n",
      "loss: 0.446945  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.572015 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.749238  [   64/60000]\n",
      "loss: 0.664277  [ 6464/60000]\n",
      "loss: 0.425079  [12864/60000]\n",
      "loss: 0.474835  [19264/60000]\n",
      "loss: 0.564930  [25664/60000]\n",
      "loss: 0.495676  [32064/60000]\n",
      "loss: 0.505905  [38464/60000]\n",
      "loss: 0.444929  [44864/60000]\n",
      "loss: 0.403416  [51264/60000]\n",
      "loss: 0.507549  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.566586 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.462733  [   64/60000]\n",
      "loss: 0.503894  [ 6464/60000]\n",
      "loss: 0.498125  [12864/60000]\n",
      "loss: 0.846192  [19264/60000]\n",
      "loss: 0.542189  [25664/60000]\n",
      "loss: 0.587251  [32064/60000]\n",
      "loss: 0.452436  [38464/60000]\n",
      "loss: 0.645342  [44864/60000]\n",
      "loss: 0.437096  [51264/60000]\n",
      "loss: 0.447070  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.558650 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.455615  [   64/60000]\n",
      "loss: 0.435721  [ 6464/60000]\n",
      "loss: 0.429636  [12864/60000]\n",
      "loss: 0.501736  [19264/60000]\n",
      "loss: 0.516958  [25664/60000]\n",
      "loss: 0.517221  [32064/60000]\n",
      "loss: 0.406161  [38464/60000]\n",
      "loss: 0.619467  [44864/60000]\n",
      "loss: 0.433224  [51264/60000]\n",
      "loss: 0.409692  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.554976 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.518721  [   64/60000]\n",
      "loss: 0.626537  [ 6464/60000]\n",
      "loss: 0.521013  [12864/60000]\n",
      "loss: 0.704989  [19264/60000]\n",
      "loss: 0.558312  [25664/60000]\n",
      "loss: 0.662568  [32064/60000]\n",
      "loss: 0.518008  [38464/60000]\n",
      "loss: 0.582303  [44864/60000]\n",
      "loss: 0.487284  [51264/60000]\n",
      "loss: 0.560391  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.551239 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.497249  [   64/60000]\n",
      "loss: 0.695861  [ 6464/60000]\n",
      "loss: 0.605007  [12864/60000]\n",
      "loss: 0.299252  [19264/60000]\n",
      "loss: 0.511359  [25664/60000]\n",
      "loss: 0.585049  [32064/60000]\n",
      "loss: 0.430905  [38464/60000]\n",
      "loss: 0.621143  [44864/60000]\n",
      "loss: 0.521569  [51264/60000]\n",
      "loss: 0.591953  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.545519 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.595899  [   64/60000]\n",
      "loss: 0.514590  [ 6464/60000]\n",
      "loss: 0.592984  [12864/60000]\n",
      "loss: 0.479848  [19264/60000]\n",
      "loss: 0.534856  [25664/60000]\n",
      "loss: 0.721691  [32064/60000]\n",
      "loss: 0.584293  [38464/60000]\n",
      "loss: 0.683163  [44864/60000]\n",
      "loss: 0.585464  [51264/60000]\n",
      "loss: 0.479393  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.539897 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.323838  [   64/60000]\n",
      "loss: 0.334519  [ 6464/60000]\n",
      "loss: 0.525301  [12864/60000]\n",
      "loss: 0.440648  [19264/60000]\n",
      "loss: 0.482410  [25664/60000]\n",
      "loss: 0.563944  [32064/60000]\n",
      "loss: 0.471334  [38464/60000]\n",
      "loss: 0.492633  [44864/60000]\n",
      "loss: 0.519218  [51264/60000]\n",
      "loss: 0.633353  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.537118 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.421333  [   64/60000]\n",
      "loss: 0.583597  [ 6464/60000]\n",
      "loss: 0.407839  [12864/60000]\n",
      "loss: 0.636275  [19264/60000]\n",
      "loss: 0.483178  [25664/60000]\n",
      "loss: 0.517925  [32064/60000]\n",
      "loss: 0.455356  [38464/60000]\n",
      "loss: 0.361207  [44864/60000]\n",
      "loss: 0.556820  [51264/60000]\n",
      "loss: 0.463814  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.531348 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.474125  [   64/60000]\n",
      "loss: 0.493879  [ 6464/60000]\n",
      "loss: 0.541131  [12864/60000]\n",
      "loss: 0.532471  [19264/60000]\n",
      "loss: 0.609851  [25664/60000]\n",
      "loss: 0.643803  [32064/60000]\n",
      "loss: 0.410923  [38464/60000]\n",
      "loss: 0.608139  [44864/60000]\n",
      "loss: 0.449095  [51264/60000]\n",
      "loss: 0.457513  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.530813 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.429493  [   64/60000]\n",
      "loss: 0.518422  [ 6464/60000]\n",
      "loss: 0.484867  [12864/60000]\n",
      "loss: 0.545430  [19264/60000]\n",
      "loss: 0.870882  [25664/60000]\n",
      "loss: 0.372910  [32064/60000]\n",
      "loss: 0.575351  [38464/60000]\n",
      "loss: 0.519058  [44864/60000]\n",
      "loss: 0.477440  [51264/60000]\n",
      "loss: 0.588095  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.527125 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.604875  [   64/60000]\n",
      "loss: 0.394046  [ 6464/60000]\n",
      "loss: 0.481379  [12864/60000]\n",
      "loss: 0.625186  [19264/60000]\n",
      "loss: 0.429589  [25664/60000]\n",
      "loss: 0.587406  [32064/60000]\n",
      "loss: 0.469590  [38464/60000]\n",
      "loss: 0.687980  [44864/60000]\n",
      "loss: 0.353385  [51264/60000]\n",
      "loss: 0.513003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.522540 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.346993  [   64/60000]\n",
      "loss: 0.467359  [ 6464/60000]\n",
      "loss: 0.421567  [12864/60000]\n",
      "loss: 0.397603  [19264/60000]\n",
      "loss: 0.679037  [25664/60000]\n",
      "loss: 0.443157  [32064/60000]\n",
      "loss: 0.473919  [38464/60000]\n",
      "loss: 0.565334  [44864/60000]\n",
      "loss: 0.484511  [51264/60000]\n",
      "loss: 0.495663  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.519006 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.534358  [   64/60000]\n",
      "loss: 0.474592  [ 6464/60000]\n",
      "loss: 0.800279  [12864/60000]\n",
      "loss: 0.498341  [19264/60000]\n",
      "loss: 0.415673  [25664/60000]\n",
      "loss: 0.437366  [32064/60000]\n",
      "loss: 0.464604  [38464/60000]\n",
      "loss: 0.438778  [44864/60000]\n",
      "loss: 0.538362  [51264/60000]\n",
      "loss: 0.448755  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.515067 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.726609  [   64/60000]\n",
      "loss: 0.537510  [ 6464/60000]\n",
      "loss: 0.540187  [12864/60000]\n",
      "loss: 0.654055  [19264/60000]\n",
      "loss: 0.335805  [25664/60000]\n",
      "loss: 0.487732  [32064/60000]\n",
      "loss: 0.348730  [38464/60000]\n",
      "loss: 0.818420  [44864/60000]\n",
      "loss: 0.644733  [51264/60000]\n",
      "loss: 0.250874  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.513407 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.490206  [   64/60000]\n",
      "loss: 0.416413  [ 6464/60000]\n",
      "loss: 0.494560  [12864/60000]\n",
      "loss: 0.543528  [19264/60000]\n",
      "loss: 0.502236  [25664/60000]\n",
      "loss: 0.487702  [32064/60000]\n",
      "loss: 0.574614  [38464/60000]\n",
      "loss: 0.463331  [44864/60000]\n",
      "loss: 0.336021  [51264/60000]\n",
      "loss: 0.489844  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.511130 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.724300  [   64/60000]\n",
      "loss: 0.457810  [ 6464/60000]\n",
      "loss: 0.517529  [12864/60000]\n",
      "loss: 0.560565  [19264/60000]\n",
      "loss: 0.713266  [25664/60000]\n",
      "loss: 0.472658  [32064/60000]\n",
      "loss: 0.477909  [38464/60000]\n",
      "loss: 0.509980  [44864/60000]\n",
      "loss: 0.499225  [51264/60000]\n",
      "loss: 0.446695  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.507833 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.510597  [   64/60000]\n",
      "loss: 0.382927  [ 6464/60000]\n",
      "loss: 0.481819  [12864/60000]\n",
      "loss: 0.544387  [19264/60000]\n",
      "loss: 0.309127  [25664/60000]\n",
      "loss: 0.390831  [32064/60000]\n",
      "loss: 0.555952  [38464/60000]\n",
      "loss: 0.434161  [44864/60000]\n",
      "loss: 0.571453  [51264/60000]\n",
      "loss: 0.576726  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.506678 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.388535  [   64/60000]\n",
      "loss: 0.443953  [ 6464/60000]\n",
      "loss: 0.511139  [12864/60000]\n",
      "loss: 0.425792  [19264/60000]\n",
      "loss: 0.374646  [25664/60000]\n",
      "loss: 0.332507  [32064/60000]\n",
      "loss: 0.782986  [38464/60000]\n",
      "loss: 0.526827  [44864/60000]\n",
      "loss: 0.416919  [51264/60000]\n",
      "loss: 0.615041  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.505810 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.364682  [   64/60000]\n",
      "loss: 0.440623  [ 6464/60000]\n",
      "loss: 0.694846  [12864/60000]\n",
      "loss: 0.412408  [19264/60000]\n",
      "loss: 0.468251  [25664/60000]\n",
      "loss: 0.424252  [32064/60000]\n",
      "loss: 0.413658  [38464/60000]\n",
      "loss: 0.594169  [44864/60000]\n",
      "loss: 0.409328  [51264/60000]\n",
      "loss: 0.617387  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.505210 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.487745  [   64/60000]\n",
      "loss: 0.594512  [ 6464/60000]\n",
      "loss: 0.335037  [12864/60000]\n",
      "loss: 0.415020  [19264/60000]\n",
      "loss: 0.503402  [25664/60000]\n",
      "loss: 0.466248  [32064/60000]\n",
      "loss: 0.419888  [38464/60000]\n",
      "loss: 0.172655  [44864/60000]\n",
      "loss: 0.329709  [51264/60000]\n",
      "loss: 0.512153  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.498853 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.595519  [   64/60000]\n",
      "loss: 0.490414  [ 6464/60000]\n",
      "loss: 0.689870  [12864/60000]\n",
      "loss: 0.446317  [19264/60000]\n",
      "loss: 0.521832  [25664/60000]\n",
      "loss: 0.424394  [32064/60000]\n",
      "loss: 0.401333  [38464/60000]\n",
      "loss: 0.465566  [44864/60000]\n",
      "loss: 0.416240  [51264/60000]\n",
      "loss: 0.374216  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.498580 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.486286  [   64/60000]\n",
      "loss: 0.708487  [ 6464/60000]\n",
      "loss: 0.435316  [12864/60000]\n",
      "loss: 0.615430  [19264/60000]\n",
      "loss: 0.344582  [25664/60000]\n",
      "loss: 0.519708  [32064/60000]\n",
      "loss: 0.551641  [38464/60000]\n",
      "loss: 0.632697  [44864/60000]\n",
      "loss: 0.392656  [51264/60000]\n",
      "loss: 0.365672  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.495576 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.491588  [   64/60000]\n",
      "loss: 0.304489  [ 6464/60000]\n",
      "loss: 0.606830  [12864/60000]\n",
      "loss: 0.405276  [19264/60000]\n",
      "loss: 0.444082  [25664/60000]\n",
      "loss: 0.367543  [32064/60000]\n",
      "loss: 0.456177  [38464/60000]\n",
      "loss: 0.483159  [44864/60000]\n",
      "loss: 0.473332  [51264/60000]\n",
      "loss: 0.395069  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.492718 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.387107  [   64/60000]\n",
      "loss: 0.385625  [ 6464/60000]\n",
      "loss: 0.435344  [12864/60000]\n",
      "loss: 0.432865  [19264/60000]\n",
      "loss: 0.351247  [25664/60000]\n",
      "loss: 0.489178  [32064/60000]\n",
      "loss: 0.452434  [38464/60000]\n",
      "loss: 0.329800  [44864/60000]\n",
      "loss: 0.508216  [51264/60000]\n",
      "loss: 0.572093  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.492423 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.633602  [   64/60000]\n",
      "loss: 0.622088  [ 6464/60000]\n",
      "loss: 0.484043  [12864/60000]\n",
      "loss: 0.562842  [19264/60000]\n",
      "loss: 0.318384  [25664/60000]\n",
      "loss: 0.515885  [32064/60000]\n",
      "loss: 0.571866  [38464/60000]\n",
      "loss: 0.606747  [44864/60000]\n",
      "loss: 0.639099  [51264/60000]\n",
      "loss: 0.554810  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.489025 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.337883  [   64/60000]\n",
      "loss: 0.397920  [ 6464/60000]\n",
      "loss: 0.479602  [12864/60000]\n",
      "loss: 0.477176  [19264/60000]\n",
      "loss: 0.383991  [25664/60000]\n",
      "loss: 0.422094  [32064/60000]\n",
      "loss: 0.470945  [38464/60000]\n",
      "loss: 0.560707  [44864/60000]\n",
      "loss: 0.481360  [51264/60000]\n",
      "loss: 0.417972  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.487056 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.358054  [   64/60000]\n",
      "loss: 0.460957  [ 6464/60000]\n",
      "loss: 0.452749  [12864/60000]\n",
      "loss: 0.466828  [19264/60000]\n",
      "loss: 0.423070  [25664/60000]\n",
      "loss: 0.390899  [32064/60000]\n",
      "loss: 0.536311  [38464/60000]\n",
      "loss: 0.314167  [44864/60000]\n",
      "loss: 0.604994  [51264/60000]\n",
      "loss: 0.522994  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.486466 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.467562  [   64/60000]\n",
      "loss: 0.334810  [ 6464/60000]\n",
      "loss: 0.649766  [12864/60000]\n",
      "loss: 0.438728  [19264/60000]\n",
      "loss: 0.541774  [25664/60000]\n",
      "loss: 0.505408  [32064/60000]\n",
      "loss: 0.472290  [38464/60000]\n",
      "loss: 0.427141  [44864/60000]\n",
      "loss: 0.311731  [51264/60000]\n",
      "loss: 0.566355  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.482761 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.553128  [   64/60000]\n",
      "loss: 0.410237  [ 6464/60000]\n",
      "loss: 0.482734  [12864/60000]\n",
      "loss: 0.413831  [19264/60000]\n",
      "loss: 0.535221  [25664/60000]\n",
      "loss: 0.439283  [32064/60000]\n",
      "loss: 0.441220  [38464/60000]\n",
      "loss: 0.383620  [44864/60000]\n",
      "loss: 0.465559  [51264/60000]\n",
      "loss: 0.513959  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.486501 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.504710  [   64/60000]\n",
      "loss: 0.547166  [ 6464/60000]\n",
      "loss: 0.394184  [12864/60000]\n",
      "loss: 0.437050  [19264/60000]\n",
      "loss: 0.592248  [25664/60000]\n",
      "loss: 0.611370  [32064/60000]\n",
      "loss: 0.776487  [38464/60000]\n",
      "loss: 0.385508  [44864/60000]\n",
      "loss: 0.436269  [51264/60000]\n",
      "loss: 0.347696  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.480116 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.499615  [   64/60000]\n",
      "loss: 0.413267  [ 6464/60000]\n",
      "loss: 0.370489  [12864/60000]\n",
      "loss: 0.352005  [19264/60000]\n",
      "loss: 0.347356  [25664/60000]\n",
      "loss: 0.590326  [32064/60000]\n",
      "loss: 0.416238  [38464/60000]\n",
      "loss: 0.312431  [44864/60000]\n",
      "loss: 0.362018  [51264/60000]\n",
      "loss: 0.389099  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.479959 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.530841  [   64/60000]\n",
      "loss: 0.321190  [ 6464/60000]\n",
      "loss: 0.530979  [12864/60000]\n",
      "loss: 0.279762  [19264/60000]\n",
      "loss: 0.557952  [25664/60000]\n",
      "loss: 0.332669  [32064/60000]\n",
      "loss: 0.409509  [38464/60000]\n",
      "loss: 0.466390  [44864/60000]\n",
      "loss: 0.473472  [51264/60000]\n",
      "loss: 0.475029  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.478425 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.405209  [   64/60000]\n",
      "loss: 0.475363  [ 6464/60000]\n",
      "loss: 0.338749  [12864/60000]\n",
      "loss: 0.426636  [19264/60000]\n",
      "loss: 0.526569  [25664/60000]\n",
      "loss: 0.392195  [32064/60000]\n",
      "loss: 0.414693  [38464/60000]\n",
      "loss: 0.473440  [44864/60000]\n",
      "loss: 0.375073  [51264/60000]\n",
      "loss: 0.342062  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.477665 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.525226  [   64/60000]\n",
      "loss: 0.458401  [ 6464/60000]\n",
      "loss: 0.542516  [12864/60000]\n",
      "loss: 0.394564  [19264/60000]\n",
      "loss: 0.389726  [25664/60000]\n",
      "loss: 0.497977  [32064/60000]\n",
      "loss: 0.458896  [38464/60000]\n",
      "loss: 0.439558  [44864/60000]\n",
      "loss: 0.420412  [51264/60000]\n",
      "loss: 0.541173  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.475351 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.520896  [   64/60000]\n",
      "loss: 0.462975  [ 6464/60000]\n",
      "loss: 0.519568  [12864/60000]\n",
      "loss: 0.370332  [19264/60000]\n",
      "loss: 0.574693  [25664/60000]\n",
      "loss: 0.484721  [32064/60000]\n",
      "loss: 0.477567  [38464/60000]\n",
      "loss: 0.562067  [44864/60000]\n",
      "loss: 0.450098  [51264/60000]\n",
      "loss: 0.514829  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.474316 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.494433  [   64/60000]\n",
      "loss: 0.432556  [ 6464/60000]\n",
      "loss: 0.482745  [12864/60000]\n",
      "loss: 0.244654  [19264/60000]\n",
      "loss: 0.356986  [25664/60000]\n",
      "loss: 0.507195  [32064/60000]\n",
      "loss: 0.506159  [38464/60000]\n",
      "loss: 0.522789  [44864/60000]\n",
      "loss: 0.472688  [51264/60000]\n",
      "loss: 0.244269  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.472786 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.469443  [   64/60000]\n",
      "loss: 0.637168  [ 6464/60000]\n",
      "loss: 0.427592  [12864/60000]\n",
      "loss: 0.309930  [19264/60000]\n",
      "loss: 0.231475  [25664/60000]\n",
      "loss: 0.351566  [32064/60000]\n",
      "loss: 0.399221  [38464/60000]\n",
      "loss: 0.505209  [44864/60000]\n",
      "loss: 0.387240  [51264/60000]\n",
      "loss: 0.257514  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.472292 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.552683  [   64/60000]\n",
      "loss: 0.654255  [ 6464/60000]\n",
      "loss: 0.414726  [12864/60000]\n",
      "loss: 0.710556  [19264/60000]\n",
      "loss: 0.341740  [25664/60000]\n",
      "loss: 0.292374  [32064/60000]\n",
      "loss: 0.340709  [38464/60000]\n",
      "loss: 0.480363  [44864/60000]\n",
      "loss: 0.291033  [51264/60000]\n",
      "loss: 0.274507  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.470405 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.554916  [   64/60000]\n",
      "loss: 0.354202  [ 6464/60000]\n",
      "loss: 0.411953  [12864/60000]\n",
      "loss: 0.411874  [19264/60000]\n",
      "loss: 0.178663  [25664/60000]\n",
      "loss: 0.415229  [32064/60000]\n",
      "loss: 0.457528  [38464/60000]\n",
      "loss: 0.390495  [44864/60000]\n",
      "loss: 0.538556  [51264/60000]\n",
      "loss: 0.303275  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.469772 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.492302  [   64/60000]\n",
      "loss: 0.379744  [ 6464/60000]\n",
      "loss: 0.485991  [12864/60000]\n",
      "loss: 0.496911  [19264/60000]\n",
      "loss: 0.372046  [25664/60000]\n",
      "loss: 0.671088  [32064/60000]\n",
      "loss: 0.464705  [38464/60000]\n",
      "loss: 0.492693  [44864/60000]\n",
      "loss: 0.336967  [51264/60000]\n",
      "loss: 0.380684  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.468501 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.483480  [   64/60000]\n",
      "loss: 0.431202  [ 6464/60000]\n",
      "loss: 0.318209  [12864/60000]\n",
      "loss: 0.427637  [19264/60000]\n",
      "loss: 0.549485  [25664/60000]\n",
      "loss: 0.363450  [32064/60000]\n",
      "loss: 0.582998  [38464/60000]\n",
      "loss: 0.562029  [44864/60000]\n",
      "loss: 0.407907  [51264/60000]\n",
      "loss: 0.368956  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.466132 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.538154  [   64/60000]\n",
      "loss: 0.436391  [ 6464/60000]\n",
      "loss: 0.335444  [12864/60000]\n",
      "loss: 0.269250  [19264/60000]\n",
      "loss: 0.351658  [25664/60000]\n",
      "loss: 0.309174  [32064/60000]\n",
      "loss: 0.549526  [38464/60000]\n",
      "loss: 0.530101  [44864/60000]\n",
      "loss: 0.402967  [51264/60000]\n",
      "loss: 0.446152  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.466591 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.340915  [   64/60000]\n",
      "loss: 0.415472  [ 6464/60000]\n",
      "loss: 0.410192  [12864/60000]\n",
      "loss: 0.503581  [19264/60000]\n",
      "loss: 0.464602  [25664/60000]\n",
      "loss: 0.453128  [32064/60000]\n",
      "loss: 0.512121  [38464/60000]\n",
      "loss: 0.582383  [44864/60000]\n",
      "loss: 0.446557  [51264/60000]\n",
      "loss: 0.571137  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.467693 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.351218  [   64/60000]\n",
      "loss: 0.451892  [ 6464/60000]\n",
      "loss: 0.434259  [12864/60000]\n",
      "loss: 0.433078  [19264/60000]\n",
      "loss: 0.821878  [25664/60000]\n",
      "loss: 0.437208  [32064/60000]\n",
      "loss: 0.481615  [38464/60000]\n",
      "loss: 0.435036  [44864/60000]\n",
      "loss: 0.294109  [51264/60000]\n",
      "loss: 0.537347  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.466104 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.431125  [   64/60000]\n",
      "loss: 0.497597  [ 6464/60000]\n",
      "loss: 0.517378  [12864/60000]\n",
      "loss: 0.374634  [19264/60000]\n",
      "loss: 0.441002  [25664/60000]\n",
      "loss: 0.410532  [32064/60000]\n",
      "loss: 0.363657  [38464/60000]\n",
      "loss: 0.401144  [44864/60000]\n",
      "loss: 0.388526  [51264/60000]\n",
      "loss: 0.349139  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.464844 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.428596  [   64/60000]\n",
      "loss: 0.327969  [ 6464/60000]\n",
      "loss: 0.487646  [12864/60000]\n",
      "loss: 0.415312  [19264/60000]\n",
      "loss: 0.582132  [25664/60000]\n",
      "loss: 0.421774  [32064/60000]\n",
      "loss: 0.568247  [38464/60000]\n",
      "loss: 0.466978  [44864/60000]\n",
      "loss: 0.521332  [51264/60000]\n",
      "loss: 0.728399  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.465375 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.314186  [   64/60000]\n",
      "loss: 0.590682  [ 6464/60000]\n",
      "loss: 0.582819  [12864/60000]\n",
      "loss: 0.481010  [19264/60000]\n",
      "loss: 0.443263  [25664/60000]\n",
      "loss: 0.461066  [32064/60000]\n",
      "loss: 0.429104  [38464/60000]\n",
      "loss: 0.416372  [44864/60000]\n",
      "loss: 0.379819  [51264/60000]\n",
      "loss: 0.498196  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.462075 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.399361  [   64/60000]\n",
      "loss: 0.483929  [ 6464/60000]\n",
      "loss: 0.405544  [12864/60000]\n",
      "loss: 0.437236  [19264/60000]\n",
      "loss: 0.370741  [25664/60000]\n",
      "loss: 0.451994  [32064/60000]\n",
      "loss: 0.275663  [38464/60000]\n",
      "loss: 0.268719  [44864/60000]\n",
      "loss: 0.545674  [51264/60000]\n",
      "loss: 0.426892  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.462780 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.569810  [   64/60000]\n",
      "loss: 0.493518  [ 6464/60000]\n",
      "loss: 0.595291  [12864/60000]\n",
      "loss: 0.509428  [19264/60000]\n",
      "loss: 0.336661  [25664/60000]\n",
      "loss: 0.591659  [32064/60000]\n",
      "loss: 0.300652  [38464/60000]\n",
      "loss: 0.269132  [44864/60000]\n",
      "loss: 0.510991  [51264/60000]\n",
      "loss: 0.577454  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.459337 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.535932  [   64/60000]\n",
      "loss: 0.271634  [ 6464/60000]\n",
      "loss: 0.542111  [12864/60000]\n",
      "loss: 0.389742  [19264/60000]\n",
      "loss: 0.264461  [25664/60000]\n",
      "loss: 0.407603  [32064/60000]\n",
      "loss: 0.412492  [38464/60000]\n",
      "loss: 0.475296  [44864/60000]\n",
      "loss: 0.646190  [51264/60000]\n",
      "loss: 0.239683  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.456908 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.424829  [   64/60000]\n",
      "loss: 0.552128  [ 6464/60000]\n",
      "loss: 0.308268  [12864/60000]\n",
      "loss: 0.383642  [19264/60000]\n",
      "loss: 0.546748  [25664/60000]\n",
      "loss: 0.415372  [32064/60000]\n",
      "loss: 0.386608  [38464/60000]\n",
      "loss: 0.371754  [44864/60000]\n",
      "loss: 0.460697  [51264/60000]\n",
      "loss: 0.435951  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.456715 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.482158  [   64/60000]\n",
      "loss: 0.559602  [ 6464/60000]\n",
      "loss: 0.611020  [12864/60000]\n",
      "loss: 0.484795  [19264/60000]\n",
      "loss: 0.383622  [25664/60000]\n",
      "loss: 0.358746  [32064/60000]\n",
      "loss: 0.441160  [38464/60000]\n",
      "loss: 0.711266  [44864/60000]\n",
      "loss: 0.325038  [51264/60000]\n",
      "loss: 0.477390  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.457343 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.522352  [   64/60000]\n",
      "loss: 0.446768  [ 6464/60000]\n",
      "loss: 0.386542  [12864/60000]\n",
      "loss: 0.500045  [19264/60000]\n",
      "loss: 0.340461  [25664/60000]\n",
      "loss: 0.382924  [32064/60000]\n",
      "loss: 0.339705  [38464/60000]\n",
      "loss: 0.498994  [44864/60000]\n",
      "loss: 0.372506  [51264/60000]\n",
      "loss: 0.419015  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.453576 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.342250  [   64/60000]\n",
      "loss: 0.618542  [ 6464/60000]\n",
      "loss: 0.310095  [12864/60000]\n",
      "loss: 0.511748  [19264/60000]\n",
      "loss: 0.378998  [25664/60000]\n",
      "loss: 0.367167  [32064/60000]\n",
      "loss: 0.504608  [38464/60000]\n",
      "loss: 0.304909  [44864/60000]\n",
      "loss: 0.451641  [51264/60000]\n",
      "loss: 0.435363  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.454575 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.499157  [   64/60000]\n",
      "loss: 0.699207  [ 6464/60000]\n",
      "loss: 0.372431  [12864/60000]\n",
      "loss: 0.485302  [19264/60000]\n",
      "loss: 0.560549  [25664/60000]\n",
      "loss: 0.335621  [32064/60000]\n",
      "loss: 0.327610  [38464/60000]\n",
      "loss: 0.515481  [44864/60000]\n",
      "loss: 0.522975  [51264/60000]\n",
      "loss: 0.292687  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.451747 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.409594  [   64/60000]\n",
      "loss: 0.331392  [ 6464/60000]\n",
      "loss: 0.582160  [12864/60000]\n",
      "loss: 0.436418  [19264/60000]\n",
      "loss: 0.459452  [25664/60000]\n",
      "loss: 0.414025  [32064/60000]\n",
      "loss: 0.355376  [38464/60000]\n",
      "loss: 0.363569  [44864/60000]\n",
      "loss: 0.352404  [51264/60000]\n",
      "loss: 0.929276  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.452053 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.430395  [   64/60000]\n",
      "loss: 0.453613  [ 6464/60000]\n",
      "loss: 0.402622  [12864/60000]\n",
      "loss: 0.472120  [19264/60000]\n",
      "loss: 0.333559  [25664/60000]\n",
      "loss: 0.411001  [32064/60000]\n",
      "loss: 0.566246  [38464/60000]\n",
      "loss: 0.454853  [44864/60000]\n",
      "loss: 0.314834  [51264/60000]\n",
      "loss: 0.392351  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.451163 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.402483  [   64/60000]\n",
      "loss: 0.401149  [ 6464/60000]\n",
      "loss: 0.478527  [12864/60000]\n",
      "loss: 0.525638  [19264/60000]\n",
      "loss: 0.387196  [25664/60000]\n",
      "loss: 0.362340  [32064/60000]\n",
      "loss: 0.437855  [38464/60000]\n",
      "loss: 0.432986  [44864/60000]\n",
      "loss: 0.458098  [51264/60000]\n",
      "loss: 0.455553  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.449863 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.510710  [   64/60000]\n",
      "loss: 0.521351  [ 6464/60000]\n",
      "loss: 0.347120  [12864/60000]\n",
      "loss: 0.439813  [19264/60000]\n",
      "loss: 0.290357  [25664/60000]\n",
      "loss: 0.293404  [32064/60000]\n",
      "loss: 0.428413  [38464/60000]\n",
      "loss: 0.337058  [44864/60000]\n",
      "loss: 0.353774  [51264/60000]\n",
      "loss: 0.473914  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.449525 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.479298  [   64/60000]\n",
      "loss: 0.437960  [ 6464/60000]\n",
      "loss: 0.383777  [12864/60000]\n",
      "loss: 0.282549  [19264/60000]\n",
      "loss: 0.533143  [25664/60000]\n",
      "loss: 0.413174  [32064/60000]\n",
      "loss: 0.388471  [38464/60000]\n",
      "loss: 0.393512  [44864/60000]\n",
      "loss: 0.482925  [51264/60000]\n",
      "loss: 0.355165  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.449096 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.283805  [   64/60000]\n",
      "loss: 0.384946  [ 6464/60000]\n",
      "loss: 0.524081  [12864/60000]\n",
      "loss: 0.478229  [19264/60000]\n",
      "loss: 0.345242  [25664/60000]\n",
      "loss: 0.531227  [32064/60000]\n",
      "loss: 0.344648  [38464/60000]\n",
      "loss: 0.177414  [44864/60000]\n",
      "loss: 0.446442  [51264/60000]\n",
      "loss: 0.337154  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.448752 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.449901  [   64/60000]\n",
      "loss: 0.534764  [ 6464/60000]\n",
      "loss: 0.439301  [12864/60000]\n",
      "loss: 0.389031  [19264/60000]\n",
      "loss: 0.388812  [25664/60000]\n",
      "loss: 0.614688  [32064/60000]\n",
      "loss: 0.429653  [38464/60000]\n",
      "loss: 0.355648  [44864/60000]\n",
      "loss: 0.585107  [51264/60000]\n",
      "loss: 0.507923  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.447015 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.468210  [   64/60000]\n",
      "loss: 0.648522  [ 6464/60000]\n",
      "loss: 0.277902  [12864/60000]\n",
      "loss: 0.364109  [19264/60000]\n",
      "loss: 0.279352  [25664/60000]\n",
      "loss: 0.352755  [32064/60000]\n",
      "loss: 0.619109  [38464/60000]\n",
      "loss: 0.240506  [44864/60000]\n",
      "loss: 0.366137  [51264/60000]\n",
      "loss: 0.471146  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.449032 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.350854  [   64/60000]\n",
      "loss: 0.423759  [ 6464/60000]\n",
      "loss: 0.603984  [12864/60000]\n",
      "loss: 0.314915  [19264/60000]\n",
      "loss: 0.529939  [25664/60000]\n",
      "loss: 0.468913  [32064/60000]\n",
      "loss: 0.470820  [38464/60000]\n",
      "loss: 0.384766  [44864/60000]\n",
      "loss: 0.419784  [51264/60000]\n",
      "loss: 0.334843  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.444297 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.276749  [   64/60000]\n",
      "loss: 0.310631  [ 6464/60000]\n",
      "loss: 0.344863  [12864/60000]\n",
      "loss: 0.604429  [19264/60000]\n",
      "loss: 0.268172  [25664/60000]\n",
      "loss: 0.311980  [32064/60000]\n",
      "loss: 0.280009  [38464/60000]\n",
      "loss: 0.389188  [44864/60000]\n",
      "loss: 0.404810  [51264/60000]\n",
      "loss: 0.408030  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.444523 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.329257  [   64/60000]\n",
      "loss: 0.416912  [ 6464/60000]\n",
      "loss: 0.615819  [12864/60000]\n",
      "loss: 0.576306  [19264/60000]\n",
      "loss: 0.420251  [25664/60000]\n",
      "loss: 0.474368  [32064/60000]\n",
      "loss: 0.486094  [38464/60000]\n",
      "loss: 0.325313  [44864/60000]\n",
      "loss: 0.356626  [51264/60000]\n",
      "loss: 0.392219  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.442190 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.291862  [   64/60000]\n",
      "loss: 0.437200  [ 6464/60000]\n",
      "loss: 0.317766  [12864/60000]\n",
      "loss: 0.463753  [19264/60000]\n",
      "loss: 0.487095  [25664/60000]\n",
      "loss: 0.395481  [32064/60000]\n",
      "loss: 0.398171  [38464/60000]\n",
      "loss: 0.360519  [44864/60000]\n",
      "loss: 0.374330  [51264/60000]\n",
      "loss: 0.367498  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.444565 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.395662  [   64/60000]\n",
      "loss: 0.326534  [ 6464/60000]\n",
      "loss: 0.392671  [12864/60000]\n",
      "loss: 0.442534  [19264/60000]\n",
      "loss: 0.374973  [25664/60000]\n",
      "loss: 0.346813  [32064/60000]\n",
      "loss: 0.314600  [38464/60000]\n",
      "loss: 0.243334  [44864/60000]\n",
      "loss: 0.286325  [51264/60000]\n",
      "loss: 0.284834  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.441090 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.484750  [   64/60000]\n",
      "loss: 0.317835  [ 6464/60000]\n",
      "loss: 0.356546  [12864/60000]\n",
      "loss: 0.489072  [19264/60000]\n",
      "loss: 0.535863  [25664/60000]\n",
      "loss: 0.420909  [32064/60000]\n",
      "loss: 0.288596  [38464/60000]\n",
      "loss: 0.516198  [44864/60000]\n",
      "loss: 0.483746  [51264/60000]\n",
      "loss: 0.419280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.440155 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.249269  [   64/60000]\n",
      "loss: 0.374028  [ 6464/60000]\n",
      "loss: 0.523414  [12864/60000]\n",
      "loss: 0.343872  [19264/60000]\n",
      "loss: 0.397028  [25664/60000]\n",
      "loss: 0.284737  [32064/60000]\n",
      "loss: 0.219970  [38464/60000]\n",
      "loss: 0.342734  [44864/60000]\n",
      "loss: 0.259246  [51264/60000]\n",
      "loss: 0.378277  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.440430 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.375215  [   64/60000]\n",
      "loss: 0.339200  [ 6464/60000]\n",
      "loss: 0.339512  [12864/60000]\n",
      "loss: 0.367881  [19264/60000]\n",
      "loss: 0.405161  [25664/60000]\n",
      "loss: 0.472686  [32064/60000]\n",
      "loss: 0.400883  [38464/60000]\n",
      "loss: 0.412840  [44864/60000]\n",
      "loss: 0.366193  [51264/60000]\n",
      "loss: 0.436165  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.438421 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.264416  [   64/60000]\n",
      "loss: 0.544061  [ 6464/60000]\n",
      "loss: 0.461250  [12864/60000]\n",
      "loss: 0.344106  [19264/60000]\n",
      "loss: 0.490646  [25664/60000]\n",
      "loss: 0.322965  [32064/60000]\n",
      "loss: 0.339217  [38464/60000]\n",
      "loss: 0.293158  [44864/60000]\n",
      "loss: 0.319451  [51264/60000]\n",
      "loss: 0.449809  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.439916 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.409421  [   64/60000]\n",
      "loss: 0.595245  [ 6464/60000]\n",
      "loss: 0.344592  [12864/60000]\n",
      "loss: 0.289120  [19264/60000]\n",
      "loss: 0.347138  [25664/60000]\n",
      "loss: 0.569838  [32064/60000]\n",
      "loss: 0.380919  [38464/60000]\n",
      "loss: 0.464290  [44864/60000]\n",
      "loss: 0.267819  [51264/60000]\n",
      "loss: 0.373880  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.437187 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar y Cargar el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de PyTorch almacenan los parámetros aprendidos en un diccionario de estado interno, llamado `state_dict`. Estos pueden ser guardados mediante el método `torch.save`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "torch.save(model.state_dict(), '/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/models/model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar los pesos del modelo, primero necesitas crear una instancia del mismo modelo y luego cargar los parámetros usando el método `load_state_dict()`.\n",
    "\n",
    "En el código a continuación, establecemos `weights_only=True` para limitar las funciones ejecutadas durante el deserializado solo a aquellas necesarias para cargar los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el modelo\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/models/model_weights.pth\", weights_only=True))\n",
    "#model.eval() # important to call before inferencing to set the dropout and batch normalization layers to evaluation mode (not applicable in this example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Al cargar los pesos del modelo, necesitamos instanciar primero la clase del modelo, porque la clase define la estructura de una red. \n",
    "- Puede que queramos guardar la estructura de esta clase junto con el modelo, en cuyo caso podemos pasar el modelo (y no `model.state_dict()`) a la función de guardado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/models/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, podemos cargar el modelo como se demuestra a continuación.\n",
    "\n",
    "**NOTA**: Guardar `state_dict` se considera la mejor práctica. Sin embargo, a continuación usamos `weights_only=False` porque esto implica cargar el modelo, que es un caso de uso heredado para `torch.save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/linar/Desktop/ML/Clases/i302/Clases/08_Clase 8: PyTorch/models/model.pth', weights_only=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i302",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
