{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regresi√≥n Lineal Regularizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øPor qu√© regularizar?\n",
    "\n",
    "- La mejor forma de entender la regresi√≥n regularizada es viendo c√≥mo y por qu√© se aplica a los m√≠nimos cuadrados ordinarios (OLS).\n",
    "- En m√≠nimos cuadrados, el objetivo es encontrar el hiperplano (por ejemplo, una l√≠nea recta en 2d) que **minimice la suma de los errores al cuadrado** $SSE = \\sum (y_i - \\hat{y_i})^2$\n",
    "- Esto significa identificar el hiperplano que minimiza las l√≠neas grises en el gr√°fico, que representan la distancia vertical entre los valores observados y los valores predichos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generamos datos\n",
    "np.random.seed(42)\n",
    "Gr_Liv_Area = np.random.uniform(900, 3000, 500)\n",
    "Sale_Price = 50000 + 100 * Gr_Liv_Area + np.random.normal(0, 50000, 500)\n",
    "\n",
    "# Ajusto regresipon lineal\n",
    "coeffs = np.polyfit(Gr_Liv_Area, Sale_Price, 1)\n",
    "regression_line = np.poly1d(coeffs)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(Gr_Liv_Area, Sale_Price, color='red', label=\"Datos reales\")\n",
    "plt.plot(sorted(Gr_Liv_Area), regression_line(sorted(Gr_Liv_Area)), color='blue', linewidth=2, label=\"Regresi√≥n lineal\")\n",
    "for x, y in zip(Gr_Liv_Area, Sale_Price):\n",
    "    plt.plot([x, x], [y, regression_line(x)], color='gray', linewidth=0.8)\n",
    "\n",
    "plt.xlabel(\"Gr_Liv_Area\")\n",
    "plt.ylabel(\"Sale_Price\")\n",
    "plt.title(\"Regresi√≥n Lineal de Precios de Venta\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recordemos entonces que la funci√≥n objetivo a minimizar es:\n",
    "\n",
    "$$ \\min \\left(  \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i}(x_i, w))^{2}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esta funci√≥n objetivo funciona bien cuando los datos cumplen con ciertos supuestos clave:  \n",
    "  1. **Relaci√≥n lineal** entre las variables.  \n",
    "  2. **M√°s observaciones ($n$) que caracter√≠sticas ($p$)** ‚Üí es decir, $n>p$.  \n",
    "  3. **Poca o ninguna multicolinealidad** entre las variables explicativas.\n",
    "\n",
    "- Muchos datasets reales (ej: data mining o estudios gen√≥micos) tienen muchas m√°s features que muestras.\n",
    "- A medida que $p$ crece, es m√°s probable que violemos algunos supuestos $\\rightarrow$ requiere considerar enfoques alternativos.\n",
    "\n",
    "### ¬øQu√© problemas trae un gran n√∫mero de features?\n",
    "\n",
    "üëé Menos interpretabilidad: Cuantas m√°s variables, m√°s dif√≠cil es entender el modelo.\n",
    "\n",
    "üëé Infinitas soluciones: Cuando $p>n$, hay infinitas soluciones para el problema de OLS.\n",
    "\n",
    "- Para abordar esto, aplicamos el principio de _\"bet on sparsity\"_ (Hastie, Tibshirani y Wainwright, 2015):\n",
    "    - Asumimos que solo un subconjunto peque√±o de caracter√≠sticas tiene el mayor impacto.\n",
    "\n",
    "### Selecci√≥n de Features\n",
    "\n",
    "####  <font color=purple>1. Hard Thresholding</font>\n",
    "Selecciona o elimina completamente una feature. M√©todos cl√°sicos incluyen:\n",
    "\n",
    "1. <font color=purple>Selecci√≥n hacia adelante (Forward Selection)</font>: \n",
    "Se empieza con un modelo vac√≠o y se van agregando features una por una, eligiendo la que m√°s mejora el ajuste del modelo.\n",
    "\n",
    "2. <font color=purple>Eliminaci√≥n hacia atr√°s</font>: \n",
    "Se empieza con todas las features y se van eliminando una por una, descartando las menos relevantes.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/forward-stepwise-algorithm.png\" alt=\"Forward Stepwise Algorithm\" width=\"370\">    \n",
    "<img src=\"img/backward-stepwise-algorithm.png\" alt=\"Forward Stepwise Algorithm\" width=\"390\">\n",
    "</div>\n",
    "\n",
    "<font color=red>Desventajas:</font>\n",
    "- Computacionalmente costoso, sobre todo con muchas variables.\n",
    "- No escala bien en conjuntos de datos grandes.\n",
    "- Decisi√≥n binaria extrema ‚Üí una caracter√≠stica est√° 100% dentro o 100% fuera, sin considerar que puede tener un efecto menor pero relevante.\n",
    "\n",
    "#### <font color=purple>2. Soft Thresholding</font>\n",
    "\n",
    "No elimina variables abruptamente, sino que reduce gradualmente sus coeficientes, permitiendo que algunas caracter√≠sticas pierdan importancia sin ser descartadas de inmediato. \n",
    "\n",
    "En algunos casos, esto lleva a que el coeficiente se vuelva exactamente cero, eliminando la variable de manera m√°s natural y optimizada.\n",
    "    \n",
    "<font color=purple>Regularizaci√≥n</font>: \n",
    "- Objetivo: Restringir el tama√±o total de los coeficientes para reducir su magnitud y fluctuaci√≥n.\n",
    "- Trade-off: Se reduce la varianza, pero el modelo deja de ser insesgado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La funci√≥n objetivo de un modelo de regresi√≥n regularizada agrega un t√©rmino de penalizaci√≥n P:\n",
    "$$ \\min \\left( SSE + P  \\right)$$\n",
    "- Este par√°metro de penalizaci√≥n restringe el tama√±o de los coeficientes:\n",
    "    - Los coeficientes solo pueden aumentar si hay una reducci√≥n proporcional en el error SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este concepto se extiende a otros modelos de la familia de Modelos Lineales Generalizados (como Regresi√≥n log√≠stica o Regresi√≥n de Poisson). \n",
    "- Obviamente, cada modelo tiene una funci√≥n de p√©rdida diferente, pero el par√°metro de penalizaci√≥n funciona igual ‚Üí restringe los coeficientes a menos que haya una reducci√≥n equivalente en la funci√≥n de p√©rdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen tres m√©todos de penalizaci√≥n comunes: \n",
    "1. **Ridge**: Penaliza los coeficientes, pero los mantiene peque√±os sin hacerlos cero.\n",
    "2. **LASSO**: Puede reducir algunos coeficientes a exactamente cero, eliminando variables irrelevantes.\n",
    "3. **Elastic Net** (ENET): Combinaci√≥n de Ridge y Lasso, √∫til cuando hay muchas variables correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresi√≥n Ridge\n",
    "- Controla los coeficientes estimados agregando un t√©rmino de penalizaci√≥n: $\\lambda $ a la funci√≥n objetivo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuente: https://bradleyboehmke.github.io/HOML/regularized-regression.html#ref-hastie2015statistical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
